{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNAoXOZ+efjPZlll+M/FQSt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyanka2454/AML/blob/main/Assignment_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Make epochs=25 wherever it is\n",
        "2.Model Summaries of each and every question\n"
      ],
      "metadata": {
        "id": "m--l4pUwWiHC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "nRLjvvy6Cliw",
        "outputId": "551f1020-9c2c-4510-bc0f-72b144817b90"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-17e08c63-0a65-4340-b430-26485a99d0b8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-17e08c63-0a65-4340-b430-26485a99d0b8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"priyankateddy\",\"key\":\"2cac501fc147aa1f2226979b8c352245\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "pmJm4wpmDUqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WfxNskdjWgBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c dogs-vs-cats\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snODx7rOD64c",
        "outputId": "122c9051-7f86-4886-99ab-aefb78d41326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dogs-vs-cats.zip to /content\n",
            " 96% 783M/812M [00:03<00:00, 241MB/s]\n",
            "100% 812M/812M [00:03<00:00, 237MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq dogs-vs-cats.zip\n",
        "\n",
        "!unzip -qq train.zip"
      ],
      "metadata": {
        "id": "aIsZNFiaD9pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define split sizes as per question requirement\n",
        "train_size = 500  # images per class\n",
        "validation_size = 250  # images per class\n",
        "test_size = 250  # images per class\n",
        "\n",
        "# Source directory\n",
        "source_dir = 'train'\n",
        "\n",
        "# Target directories\n",
        "base_dir = '/content/cats_and_dogs'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Make directories if not present\n",
        "os.makedirs(train_dir + '/cats', exist_ok=True)\n",
        "os.makedirs(train_dir + '/dogs', exist_ok=True)\n",
        "os.makedirs(validation_dir + '/cats', exist_ok=True)\n",
        "os.makedirs(validation_dir + '/dogs', exist_ok=True)\n",
        "os.makedirs(test_dir + '/cats', exist_ok=True)\n",
        "os.makedirs(test_dir + '/dogs', exist_ok=True)\n",
        "\n",
        "# Get lists of all images\n",
        "all_images = os.listdir(source_dir)\n",
        "cat_images = [img for img in all_images if 'cat' in img]\n",
        "dog_images = [img for img in all_images if 'dog' in img]\n",
        "\n",
        "# Copy images to target directories\n",
        "def copy_images(images, start, end, target_dir):\n",
        "    for img in images[start:end]:\n",
        "        shutil.copy(os.path.join(source_dir, img), target_dir)\n",
        "\n",
        "# Assign images to train, validation, and test sets\n",
        "copy_images(cat_images, 0, train_size, os.path.join(train_dir, 'cats'))\n",
        "copy_images(dog_images, 0, train_size, os.path.join(train_dir, 'dogs'))\n",
        "\n",
        "copy_images(cat_images, train_size, train_size + validation_size, os.path.join(validation_dir, 'cats'))\n",
        "copy_images(dog_images, train_size, train_size + validation_size, os.path.join(validation_dir, 'dogs'))\n",
        "\n",
        "copy_images(cat_images, train_size + validation_size, train_size + validation_size + test_size, os.path.join(test_dir, 'cats'))\n",
        "copy_images(dog_images, train_size + validation_size, train_size + validation_size + test_size, os.path.join(test_dir, 'dogs'))\n"
      ],
      "metadata": {
        "id": "YzCEr7qLbWiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Directories\n",
        "train_dir = '/content/cats_and_dogs/train'\n",
        "validation_dir = '/content/cats_and_dogs/validation'\n",
        "test_dir = '/content/cats_and_dogs/test'\n",
        "\n",
        "# Function to count images in each class directory\n",
        "def count_images(directory):\n",
        "    cat_count = len(os.listdir(os.path.join(directory, 'cats')))\n",
        "    dog_count = len(os.listdir(os.path.join(directory, 'dogs')))\n",
        "    return cat_count, dog_count\n",
        "\n",
        "# Verify training set\n",
        "train_cat_count, train_dog_count = count_images(train_dir)\n",
        "print(f\"Training set: {train_cat_count} cats, {train_dog_count} dogs (Total: {train_cat_count + train_dog_count})\")\n",
        "\n",
        "# Verify validation set\n",
        "val_cat_count, val_dog_count = count_images(validation_dir)\n",
        "print(f\"Validation set: {val_cat_count} cats, {val_dog_count} dogs (Total: {val_cat_count + val_dog_count})\")\n",
        "\n",
        "# Verify test set\n",
        "test_cat_count, test_dog_count = count_images(test_dir)\n",
        "print(f\"Test set: {test_cat_count} cats, {test_dog_count} dogs (Total: {test_cat_count + test_dog_count})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDn7Lf_rfrmw",
        "outputId": "3a04f912-c41e-4714-c364-0d6a7e2638da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 500 cats, 500 dogs (Total: 1000)\n",
            "Validation set: 250 cats, 250 dogs (Total: 500)\n",
            "Test set: 250 cats, 250 dogs (Total: 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question - 1\n",
        "#Base Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Basic data preprocessing (rescaling only)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Directory paths\n",
        "train_dir = '/content/cats_and_dogs/train'\n",
        "validation_dir = '/content/cats_and_dogs/validation'\n",
        "test_dir = '/content/cats_and_dogs/test'\n",
        "\n",
        "# Data generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Simple CNN Model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model (baseline)\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    epochs=25,  # You can start with fewer epochs for initial testing\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=len(validation_generator)\n",
        ")\n",
        "\n",
        "# Evaluate on test data\n",
        "test_loss, test_acc = model.evaluate(test_generator, steps=len(test_generator))\n",
        "print(f\"Test accuracy (Baseline): {test_acc}\")\n"
      ],
      "metadata": {
        "id": "IxJAfq0RaFcC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd1fea1c-c19c-40bb-c10f-0c13b0e6213f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1000 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "32/32 [==============================] - 9s 241ms/step - loss: 0.8186 - accuracy: 0.5360 - val_loss: 0.7198 - val_accuracy: 0.5000\n",
            "Epoch 2/25\n",
            "32/32 [==============================] - 7s 233ms/step - loss: 0.6801 - accuracy: 0.5690 - val_loss: 0.6670 - val_accuracy: 0.5760\n",
            "Epoch 3/25\n",
            "32/32 [==============================] - 7s 230ms/step - loss: 0.6084 - accuracy: 0.6760 - val_loss: 0.6645 - val_accuracy: 0.6260\n",
            "Epoch 4/25\n",
            "32/32 [==============================] - 7s 226ms/step - loss: 0.5246 - accuracy: 0.7440 - val_loss: 0.6356 - val_accuracy: 0.6620\n",
            "Epoch 5/25\n",
            "32/32 [==============================] - 7s 233ms/step - loss: 0.4134 - accuracy: 0.8190 - val_loss: 0.6924 - val_accuracy: 0.6540\n",
            "Epoch 6/25\n",
            "32/32 [==============================] - 7s 230ms/step - loss: 0.3361 - accuracy: 0.8520 - val_loss: 0.7893 - val_accuracy: 0.6540\n",
            "Epoch 7/25\n",
            "32/32 [==============================] - 7s 231ms/step - loss: 0.2505 - accuracy: 0.9040 - val_loss: 0.9247 - val_accuracy: 0.6380\n",
            "Epoch 8/25\n",
            "32/32 [==============================] - 7s 230ms/step - loss: 0.1681 - accuracy: 0.9370 - val_loss: 1.1740 - val_accuracy: 0.6400\n",
            "Epoch 9/25\n",
            "32/32 [==============================] - 7s 232ms/step - loss: 0.0914 - accuracy: 0.9720 - val_loss: 1.3228 - val_accuracy: 0.6500\n",
            "Epoch 10/25\n",
            "32/32 [==============================] - 7s 232ms/step - loss: 0.0551 - accuracy: 0.9910 - val_loss: 1.6150 - val_accuracy: 0.6640\n",
            "Epoch 11/25\n",
            "32/32 [==============================] - 8s 234ms/step - loss: 0.0545 - accuracy: 0.9820 - val_loss: 1.8290 - val_accuracy: 0.6500\n",
            "Epoch 12/25\n",
            "32/32 [==============================] - 7s 230ms/step - loss: 0.0416 - accuracy: 0.9850 - val_loss: 2.1282 - val_accuracy: 0.6320\n",
            "Epoch 13/25\n",
            "32/32 [==============================] - 8s 235ms/step - loss: 0.0173 - accuracy: 0.9970 - val_loss: 2.1061 - val_accuracy: 0.6380\n",
            "Epoch 14/25\n",
            "32/32 [==============================] - 7s 230ms/step - loss: 0.0066 - accuracy: 0.9990 - val_loss: 2.5724 - val_accuracy: 0.6460\n",
            "Epoch 15/25\n",
            "32/32 [==============================] - 8s 235ms/step - loss: 0.0324 - accuracy: 0.9880 - val_loss: 2.3969 - val_accuracy: 0.6180\n",
            "Epoch 16/25\n",
            "32/32 [==============================] - 7s 231ms/step - loss: 0.0474 - accuracy: 0.9850 - val_loss: 2.0335 - val_accuracy: 0.6200\n",
            "Epoch 17/25\n",
            "32/32 [==============================] - 8s 235ms/step - loss: 0.0878 - accuracy: 0.9740 - val_loss: 1.8522 - val_accuracy: 0.6460\n",
            "Epoch 18/25\n",
            "32/32 [==============================] - 7s 228ms/step - loss: 0.0606 - accuracy: 0.9770 - val_loss: 2.2563 - val_accuracy: 0.6240\n",
            "Epoch 19/25\n",
            "32/32 [==============================] - 8s 235ms/step - loss: 0.0783 - accuracy: 0.9730 - val_loss: 2.2823 - val_accuracy: 0.6400\n",
            "Epoch 20/25\n",
            "32/32 [==============================] - 7s 228ms/step - loss: 0.0343 - accuracy: 0.9920 - val_loss: 2.1999 - val_accuracy: 0.6520\n",
            "Epoch 21/25\n",
            "32/32 [==============================] - 7s 228ms/step - loss: 0.0067 - accuracy: 0.9990 - val_loss: 2.4923 - val_accuracy: 0.6580\n",
            "Epoch 22/25\n",
            "32/32 [==============================] - 8s 235ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.6517 - val_accuracy: 0.6480\n",
            "Epoch 23/25\n",
            "32/32 [==============================] - 7s 229ms/step - loss: 5.7492e-04 - accuracy: 1.0000 - val_loss: 2.7645 - val_accuracy: 0.6460\n",
            "Epoch 24/25\n",
            "32/32 [==============================] - 7s 232ms/step - loss: 3.6717e-04 - accuracy: 1.0000 - val_loss: 2.8356 - val_accuracy: 0.6400\n",
            "Epoch 25/25\n",
            "32/32 [==============================] - 8s 237ms/step - loss: 2.7804e-04 - accuracy: 1.0000 - val_loss: 2.8938 - val_accuracy: 0.6440\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 2.3044 - accuracy: 0.7060\n",
            "Test accuracy (Baseline): 0.7059999704360962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Use any technique\n",
        "to reduce overfitting and improve performance in developing a network that you train\n",
        "from scratch. What performance did you achieve?"
      ],
      "metadata": {
        "id": "4sA-wRjviaAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation for training\n",
        "train_datagen_augmented = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Validation and test data remain the same (only rescaling)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load augmented data for training\n",
        "train_generator_augmented = train_datagen_augmented.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Train the model with augmented data\n",
        "history_augmented = model.fit(\n",
        "    train_generator_augmented,\n",
        "    steps_per_epoch=len(train_generator_augmented),\n",
        "    epochs=25,  # Train for the same number of epochs as baseline\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=len(validation_generator)\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss_augmented, test_acc_augmented = model.evaluate(test_generator, steps=len(test_generator))\n",
        "print(f\"Test accuracy with Data Augmentation: {test_acc_augmented}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSVjvkZ4aJqZ",
        "outputId": "d22d0a4b-46e7-4a0a-fe80-99c17a4a8f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1000 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "32/32 [==============================] - 9s 280ms/step - loss: 0.9361 - accuracy: 0.5410 - val_loss: 0.6813 - val_accuracy: 0.6360\n",
            "Epoch 2/25\n",
            "32/32 [==============================] - 9s 279ms/step - loss: 0.6837 - accuracy: 0.5520 - val_loss: 0.6545 - val_accuracy: 0.6180\n",
            "Epoch 3/25\n",
            "32/32 [==============================] - 9s 280ms/step - loss: 0.6684 - accuracy: 0.5910 - val_loss: 0.6441 - val_accuracy: 0.6520\n",
            "Epoch 4/25\n",
            "32/32 [==============================] - 9s 277ms/step - loss: 0.6589 - accuracy: 0.6280 - val_loss: 0.6336 - val_accuracy: 0.6500\n",
            "Epoch 5/25\n",
            "32/32 [==============================] - 9s 277ms/step - loss: 0.6337 - accuracy: 0.6320 - val_loss: 0.6106 - val_accuracy: 0.6880\n",
            "Epoch 6/25\n",
            "32/32 [==============================] - 9s 281ms/step - loss: 0.6302 - accuracy: 0.6310 - val_loss: 0.6284 - val_accuracy: 0.6620\n",
            "Epoch 7/25\n",
            "32/32 [==============================] - 9s 277ms/step - loss: 0.6157 - accuracy: 0.6730 - val_loss: 0.6247 - val_accuracy: 0.6760\n",
            "Epoch 8/25\n",
            "32/32 [==============================] - 9s 278ms/step - loss: 0.6027 - accuracy: 0.6870 - val_loss: 0.6353 - val_accuracy: 0.6960\n",
            "Epoch 9/25\n",
            "32/32 [==============================] - 9s 277ms/step - loss: 0.5923 - accuracy: 0.6830 - val_loss: 0.5708 - val_accuracy: 0.7160\n",
            "Epoch 10/25\n",
            "32/32 [==============================] - 9s 279ms/step - loss: 0.5885 - accuracy: 0.6830 - val_loss: 0.5994 - val_accuracy: 0.7000\n",
            "Epoch 11/25\n",
            "32/32 [==============================] - 9s 279ms/step - loss: 0.5994 - accuracy: 0.6890 - val_loss: 0.5792 - val_accuracy: 0.7060\n",
            "Epoch 12/25\n",
            "32/32 [==============================] - 9s 280ms/step - loss: 0.5776 - accuracy: 0.6870 - val_loss: 0.5712 - val_accuracy: 0.7140\n",
            "Epoch 13/25\n",
            "32/32 [==============================] - 9s 278ms/step - loss: 0.5827 - accuracy: 0.6860 - val_loss: 0.6412 - val_accuracy: 0.6880\n",
            "Epoch 14/25\n",
            "32/32 [==============================] - 9s 276ms/step - loss: 0.5949 - accuracy: 0.6720 - val_loss: 0.6010 - val_accuracy: 0.7280\n",
            "Epoch 15/25\n",
            "32/32 [==============================] - 9s 280ms/step - loss: 0.5589 - accuracy: 0.7280 - val_loss: 0.6503 - val_accuracy: 0.6440\n",
            "Epoch 16/25\n",
            "32/32 [==============================] - 9s 279ms/step - loss: 0.5575 - accuracy: 0.7220 - val_loss: 0.6045 - val_accuracy: 0.7160\n",
            "Epoch 17/25\n",
            "32/32 [==============================] - 9s 277ms/step - loss: 0.5561 - accuracy: 0.7000 - val_loss: 0.6593 - val_accuracy: 0.6780\n",
            "Epoch 18/25\n",
            "32/32 [==============================] - 9s 280ms/step - loss: 0.5546 - accuracy: 0.7100 - val_loss: 0.5593 - val_accuracy: 0.7340\n",
            "Epoch 19/25\n",
            "32/32 [==============================] - 9s 274ms/step - loss: 0.5456 - accuracy: 0.7190 - val_loss: 0.5946 - val_accuracy: 0.7340\n",
            "Epoch 20/25\n",
            "32/32 [==============================] - 9s 277ms/step - loss: 0.5697 - accuracy: 0.7110 - val_loss: 0.6213 - val_accuracy: 0.7100\n",
            "Epoch 21/25\n",
            "32/32 [==============================] - 9s 280ms/step - loss: 0.5236 - accuracy: 0.7410 - val_loss: 0.5685 - val_accuracy: 0.7500\n",
            "Epoch 22/25\n",
            "32/32 [==============================] - 9s 282ms/step - loss: 0.5538 - accuracy: 0.7260 - val_loss: 0.5436 - val_accuracy: 0.7460\n",
            "Epoch 23/25\n",
            "32/32 [==============================] - 9s 290ms/step - loss: 0.5409 - accuracy: 0.7370 - val_loss: 0.5573 - val_accuracy: 0.7380\n",
            "Epoch 24/25\n",
            "32/32 [==============================] - 9s 275ms/step - loss: 0.5384 - accuracy: 0.7180 - val_loss: 0.5308 - val_accuracy: 0.7640\n",
            "Epoch 25/25\n",
            "32/32 [==============================] - 9s 277ms/step - loss: 0.5439 - accuracy: 0.7250 - val_loss: 0.5482 - val_accuracy: 0.7220\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5356 - accuracy: 0.7020\n",
            "Test accuracy with Data Augmentation: 0.7020000219345093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Define the model with Dropout\n",
        "model_with_dropout = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),  # Dropout to prevent overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_with_dropout.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with dropout\n",
        "history_dropout = model_with_dropout.fit(\n",
        "    train_generator_augmented,\n",
        "    steps_per_epoch=len(train_generator_augmented),\n",
        "    epochs=25,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=len(validation_generator)\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss_dropout, test_acc_dropout = model_with_dropout.evaluate(test_generator, steps=len(test_generator))\n",
        "print(f\"Test accuracy with Data Augmentation and Dropout: {test_acc_dropout}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mM0d_nrhyIQ",
        "outputId": "568c1d95-fe42-45ab-99c5-029b60745867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "32/32 [==============================] - 10s 283ms/step - loss: 0.8168 - accuracy: 0.5040 - val_loss: 0.6905 - val_accuracy: 0.5620\n",
            "Epoch 2/25\n",
            "32/32 [==============================] - 9s 275ms/step - loss: 0.6887 - accuracy: 0.5490 - val_loss: 0.6805 - val_accuracy: 0.5840\n",
            "Epoch 3/25\n",
            "32/32 [==============================] - 9s 273ms/step - loss: 0.6801 - accuracy: 0.5680 - val_loss: 0.6768 - val_accuracy: 0.6280\n",
            "Epoch 4/25\n",
            "32/32 [==============================] - 9s 272ms/step - loss: 0.6841 - accuracy: 0.5470 - val_loss: 0.6704 - val_accuracy: 0.5840\n",
            "Epoch 5/25\n",
            "32/32 [==============================] - 9s 278ms/step - loss: 0.6751 - accuracy: 0.6030 - val_loss: 0.6987 - val_accuracy: 0.5440\n",
            "Epoch 6/25\n",
            "32/32 [==============================] - 9s 276ms/step - loss: 0.6775 - accuracy: 0.5920 - val_loss: 0.6583 - val_accuracy: 0.6100\n",
            "Epoch 7/25\n",
            "32/32 [==============================] - 9s 277ms/step - loss: 0.6561 - accuracy: 0.6150 - val_loss: 0.6728 - val_accuracy: 0.5880\n",
            "Epoch 8/25\n",
            "32/32 [==============================] - 9s 277ms/step - loss: 0.6580 - accuracy: 0.5950 - val_loss: 0.6497 - val_accuracy: 0.6380\n",
            "Epoch 9/25\n",
            "32/32 [==============================] - 9s 274ms/step - loss: 0.6497 - accuracy: 0.6290 - val_loss: 0.6213 - val_accuracy: 0.6620\n",
            "Epoch 10/25\n",
            "32/32 [==============================] - 9s 273ms/step - loss: 0.6475 - accuracy: 0.6200 - val_loss: 0.6489 - val_accuracy: 0.6460\n",
            "Epoch 11/25\n",
            "32/32 [==============================] - 9s 273ms/step - loss: 0.6375 - accuracy: 0.6430 - val_loss: 0.6418 - val_accuracy: 0.6300\n",
            "Epoch 12/25\n",
            "32/32 [==============================] - 9s 278ms/step - loss: 0.6213 - accuracy: 0.6610 - val_loss: 0.7197 - val_accuracy: 0.5800\n",
            "Epoch 13/25\n",
            "32/32 [==============================] - 9s 277ms/step - loss: 0.6321 - accuracy: 0.6530 - val_loss: 0.5990 - val_accuracy: 0.6880\n",
            "Epoch 14/25\n",
            "32/32 [==============================] - 9s 279ms/step - loss: 0.6205 - accuracy: 0.6590 - val_loss: 0.6057 - val_accuracy: 0.6960\n",
            "Epoch 15/25\n",
            "32/32 [==============================] - 9s 277ms/step - loss: 0.6124 - accuracy: 0.6770 - val_loss: 0.6397 - val_accuracy: 0.6440\n",
            "Epoch 16/25\n",
            "32/32 [==============================] - 9s 272ms/step - loss: 0.6036 - accuracy: 0.7000 - val_loss: 0.6407 - val_accuracy: 0.6320\n",
            "Epoch 17/25\n",
            "32/32 [==============================] - 9s 276ms/step - loss: 0.6099 - accuracy: 0.6750 - val_loss: 0.5832 - val_accuracy: 0.7000\n",
            "Epoch 18/25\n",
            "32/32 [==============================] - 9s 274ms/step - loss: 0.5866 - accuracy: 0.6940 - val_loss: 0.5815 - val_accuracy: 0.7080\n",
            "Epoch 19/25\n",
            "32/32 [==============================] - 9s 275ms/step - loss: 0.5776 - accuracy: 0.6890 - val_loss: 0.5610 - val_accuracy: 0.7500\n",
            "Epoch 20/25\n",
            "32/32 [==============================] - 9s 274ms/step - loss: 0.5705 - accuracy: 0.6900 - val_loss: 0.5885 - val_accuracy: 0.6620\n",
            "Epoch 21/25\n",
            "32/32 [==============================] - 9s 272ms/step - loss: 0.5900 - accuracy: 0.6660 - val_loss: 0.5681 - val_accuracy: 0.7120\n",
            "Epoch 22/25\n",
            "32/32 [==============================] - 9s 275ms/step - loss: 0.5990 - accuracy: 0.6790 - val_loss: 0.5591 - val_accuracy: 0.7240\n",
            "Epoch 23/25\n",
            "32/32 [==============================] - 9s 273ms/step - loss: 0.5848 - accuracy: 0.6840 - val_loss: 0.5668 - val_accuracy: 0.7160\n",
            "Epoch 24/25\n",
            "32/32 [==============================] - 9s 272ms/step - loss: 0.5969 - accuracy: 0.6860 - val_loss: 0.5601 - val_accuracy: 0.6920\n",
            "Epoch 25/25\n",
            "32/32 [==============================] - 9s 272ms/step - loss: 0.5773 - accuracy: 0.6900 - val_loss: 0.5729 - val_accuracy: 0.7160\n",
            "16/16 [==============================] - 1s 55ms/step - loss: 0.5449 - accuracy: 0.7080\n",
            "Test accuracy with Data Augmentation and Dropout: 0.7080000042915344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "# Define the model with Batch Normalization\n",
        "model_with_batchnorm = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_with_batchnorm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with batch normalization\n",
        "history_batchnorm = model_with_batchnorm.fit(\n",
        "    train_generator_augmented,\n",
        "    steps_per_epoch=len(train_generator_augmented),\n",
        "    epochs=25,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=len(validation_generator)\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss_batchnorm, test_acc_batchnorm = model_with_batchnorm.evaluate(test_generator, steps=len(test_generator))\n",
        "print(f\"Test accuracy with Data Augmentation, Dropout, and Batch Normalization: {test_acc_batchnorm}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3rYxTGhh1pZ",
        "outputId": "e066689c-8625-4f9c-dc1b-43d81e6de172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "32/32 [==============================] - 12s 329ms/step - loss: 7.1288 - accuracy: 0.5300 - val_loss: 4.4716 - val_accuracy: 0.5020\n",
            "Epoch 2/25\n",
            "32/32 [==============================] - 10s 315ms/step - loss: 3.4645 - accuracy: 0.5680 - val_loss: 7.5162 - val_accuracy: 0.5000\n",
            "Epoch 3/25\n",
            "32/32 [==============================] - 10s 308ms/step - loss: 1.3108 - accuracy: 0.5620 - val_loss: 7.1414 - val_accuracy: 0.4980\n",
            "Epoch 4/25\n",
            "32/32 [==============================] - 10s 322ms/step - loss: 0.7718 - accuracy: 0.6100 - val_loss: 7.6391 - val_accuracy: 0.5000\n",
            "Epoch 5/25\n",
            "32/32 [==============================] - 10s 314ms/step - loss: 0.7135 - accuracy: 0.6170 - val_loss: 7.9006 - val_accuracy: 0.5020\n",
            "Epoch 6/25\n",
            "32/32 [==============================] - 10s 311ms/step - loss: 0.7143 - accuracy: 0.6290 - val_loss: 4.0690 - val_accuracy: 0.5060\n",
            "Epoch 7/25\n",
            "32/32 [==============================] - 10s 312ms/step - loss: 0.6566 - accuracy: 0.6280 - val_loss: 2.7513 - val_accuracy: 0.5020\n",
            "Epoch 8/25\n",
            "32/32 [==============================] - 10s 313ms/step - loss: 0.6225 - accuracy: 0.6400 - val_loss: 3.0277 - val_accuracy: 0.4960\n",
            "Epoch 9/25\n",
            "32/32 [==============================] - 10s 315ms/step - loss: 0.6217 - accuracy: 0.6690 - val_loss: 2.1265 - val_accuracy: 0.5120\n",
            "Epoch 10/25\n",
            "32/32 [==============================] - 10s 311ms/step - loss: 0.6368 - accuracy: 0.6520 - val_loss: 1.1617 - val_accuracy: 0.5460\n",
            "Epoch 11/25\n",
            "32/32 [==============================] - 10s 312ms/step - loss: 0.6220 - accuracy: 0.6500 - val_loss: 0.9279 - val_accuracy: 0.5720\n",
            "Epoch 12/25\n",
            "32/32 [==============================] - 10s 315ms/step - loss: 0.6170 - accuracy: 0.6810 - val_loss: 1.0163 - val_accuracy: 0.5720\n",
            "Epoch 13/25\n",
            "32/32 [==============================] - 10s 310ms/step - loss: 0.6143 - accuracy: 0.6440 - val_loss: 1.0476 - val_accuracy: 0.5560\n",
            "Epoch 14/25\n",
            "32/32 [==============================] - 10s 314ms/step - loss: 0.6143 - accuracy: 0.6520 - val_loss: 1.0717 - val_accuracy: 0.5700\n",
            "Epoch 15/25\n",
            "32/32 [==============================] - 10s 314ms/step - loss: 0.6026 - accuracy: 0.6780 - val_loss: 0.8642 - val_accuracy: 0.6120\n",
            "Epoch 16/25\n",
            "32/32 [==============================] - 10s 311ms/step - loss: 0.6235 - accuracy: 0.6570 - val_loss: 0.8637 - val_accuracy: 0.6160\n",
            "Epoch 17/25\n",
            "32/32 [==============================] - 10s 308ms/step - loss: 0.6165 - accuracy: 0.6650 - val_loss: 0.8006 - val_accuracy: 0.6440\n",
            "Epoch 18/25\n",
            "32/32 [==============================] - 10s 310ms/step - loss: 0.6236 - accuracy: 0.6530 - val_loss: 0.6785 - val_accuracy: 0.6440\n",
            "Epoch 19/25\n",
            "32/32 [==============================] - 10s 310ms/step - loss: 0.6302 - accuracy: 0.6670 - val_loss: 1.3808 - val_accuracy: 0.5960\n",
            "Epoch 20/25\n",
            "32/32 [==============================] - 10s 323ms/step - loss: 0.6083 - accuracy: 0.6710 - val_loss: 0.7896 - val_accuracy: 0.6540\n",
            "Epoch 21/25\n",
            "32/32 [==============================] - 10s 309ms/step - loss: 0.6061 - accuracy: 0.6680 - val_loss: 0.7318 - val_accuracy: 0.6960\n",
            "Epoch 22/25\n",
            "32/32 [==============================] - 10s 309ms/step - loss: 0.5654 - accuracy: 0.6930 - val_loss: 0.7023 - val_accuracy: 0.7060\n",
            "Epoch 23/25\n",
            "32/32 [==============================] - 10s 310ms/step - loss: 0.5801 - accuracy: 0.7150 - val_loss: 0.7047 - val_accuracy: 0.6780\n",
            "Epoch 24/25\n",
            "32/32 [==============================] - 10s 311ms/step - loss: 0.6098 - accuracy: 0.6830 - val_loss: 0.6704 - val_accuracy: 0.6780\n",
            "Epoch 25/25\n",
            "32/32 [==============================] - 10s 311ms/step - loss: 0.5997 - accuracy: 0.6930 - val_loss: 0.7648 - val_accuracy: 0.6160\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.6844 - accuracy: 0.6060\n",
            "Test accuracy with Data Augmentation, Dropout, and Batch Normalization: 0.6060000061988831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Learning rate scheduler callback\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Train the model with learning rate scheduler\n",
        "history_final = model_with_batchnorm.fit(\n",
        "    train_generator_augmented,\n",
        "    steps_per_epoch=len(train_generator_augmented),\n",
        "    epochs=25,  # Use more epochs to allow the scheduler to take effect\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=len(validation_generator),\n",
        "    callbacks=[lr_scheduler]\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss_final, test_acc_final = model_with_batchnorm.evaluate(test_generator, steps=len(test_generator))\n",
        "print(f\"Test accuracy with Data Augmentation, Dropout, Batch Normalization, and Learning Rate Scheduling: {test_acc_final}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwCiiKhch5gG",
        "outputId": "8e0a2b7a-19d2-4a93-b2b1-e880b310c893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "32/32 [==============================] - 10s 315ms/step - loss: 0.5878 - accuracy: 0.6770 - val_loss: 0.7208 - val_accuracy: 0.5920 - lr: 0.0010\n",
            "Epoch 2/25\n",
            "32/32 [==============================] - 10s 307ms/step - loss: 0.5807 - accuracy: 0.7040 - val_loss: 0.6650 - val_accuracy: 0.7200 - lr: 0.0010\n",
            "Epoch 3/25\n",
            "32/32 [==============================] - 10s 307ms/step - loss: 0.5913 - accuracy: 0.6840 - val_loss: 0.6917 - val_accuracy: 0.6660 - lr: 0.0010\n",
            "Epoch 4/25\n",
            "32/32 [==============================] - 10s 307ms/step - loss: 0.5790 - accuracy: 0.6840 - val_loss: 0.6584 - val_accuracy: 0.6300 - lr: 0.0010\n",
            "Epoch 5/25\n",
            "32/32 [==============================] - 10s 310ms/step - loss: 0.5912 - accuracy: 0.7130 - val_loss: 0.6399 - val_accuracy: 0.6400 - lr: 0.0010\n",
            "Epoch 6/25\n",
            "32/32 [==============================] - 10s 310ms/step - loss: 0.5980 - accuracy: 0.6980 - val_loss: 0.6555 - val_accuracy: 0.6240 - lr: 0.0010\n",
            "Epoch 7/25\n",
            "32/32 [==============================] - 10s 309ms/step - loss: 0.5897 - accuracy: 0.6940 - val_loss: 0.6590 - val_accuracy: 0.6580 - lr: 0.0010\n",
            "Epoch 8/25\n",
            "32/32 [==============================] - 10s 309ms/step - loss: 0.5818 - accuracy: 0.6910 - val_loss: 0.5925 - val_accuracy: 0.7360 - lr: 0.0010\n",
            "Epoch 9/25\n",
            "32/32 [==============================] - 10s 305ms/step - loss: 0.5902 - accuracy: 0.6860 - val_loss: 0.6773 - val_accuracy: 0.6440 - lr: 0.0010\n",
            "Epoch 10/25\n",
            "32/32 [==============================] - 10s 322ms/step - loss: 0.5754 - accuracy: 0.6800 - val_loss: 0.8831 - val_accuracy: 0.6000 - lr: 0.0010\n",
            "Epoch 11/25\n",
            "32/32 [==============================] - 10s 313ms/step - loss: 0.5637 - accuracy: 0.6970 - val_loss: 0.8297 - val_accuracy: 0.6300 - lr: 0.0010\n",
            "Epoch 12/25\n",
            "32/32 [==============================] - 10s 308ms/step - loss: 0.5499 - accuracy: 0.7270 - val_loss: 0.7577 - val_accuracy: 0.6380 - lr: 5.0000e-04\n",
            "Epoch 13/25\n",
            "32/32 [==============================] - 10s 307ms/step - loss: 0.5361 - accuracy: 0.7260 - val_loss: 0.7077 - val_accuracy: 0.6360 - lr: 5.0000e-04\n",
            "Epoch 14/25\n",
            "32/32 [==============================] - 10s 306ms/step - loss: 0.5409 - accuracy: 0.7330 - val_loss: 0.6982 - val_accuracy: 0.7320 - lr: 5.0000e-04\n",
            "Epoch 15/25\n",
            "32/32 [==============================] - 10s 313ms/step - loss: 0.5245 - accuracy: 0.7310 - val_loss: 0.5882 - val_accuracy: 0.7220 - lr: 2.5000e-04\n",
            "Epoch 16/25\n",
            "32/32 [==============================] - 10s 308ms/step - loss: 0.5075 - accuracy: 0.7310 - val_loss: 0.5777 - val_accuracy: 0.7320 - lr: 2.5000e-04\n",
            "Epoch 17/25\n",
            "32/32 [==============================] - 10s 312ms/step - loss: 0.4997 - accuracy: 0.7390 - val_loss: 0.5672 - val_accuracy: 0.7400 - lr: 2.5000e-04\n",
            "Epoch 18/25\n",
            "32/32 [==============================] - 10s 309ms/step - loss: 0.4883 - accuracy: 0.7620 - val_loss: 0.5636 - val_accuracy: 0.7360 - lr: 2.5000e-04\n",
            "Epoch 19/25\n",
            "32/32 [==============================] - 10s 308ms/step - loss: 0.5071 - accuracy: 0.7570 - val_loss: 0.5638 - val_accuracy: 0.7380 - lr: 2.5000e-04\n",
            "Epoch 20/25\n",
            "32/32 [==============================] - 10s 307ms/step - loss: 0.4971 - accuracy: 0.7570 - val_loss: 0.5642 - val_accuracy: 0.7600 - lr: 2.5000e-04\n",
            "Epoch 21/25\n",
            "32/32 [==============================] - 10s 309ms/step - loss: 0.4691 - accuracy: 0.7600 - val_loss: 0.5898 - val_accuracy: 0.7300 - lr: 2.5000e-04\n",
            "Epoch 22/25\n",
            "32/32 [==============================] - 10s 308ms/step - loss: 0.4880 - accuracy: 0.7610 - val_loss: 0.5416 - val_accuracy: 0.7600 - lr: 1.2500e-04\n",
            "Epoch 23/25\n",
            "32/32 [==============================] - 10s 314ms/step - loss: 0.4744 - accuracy: 0.7630 - val_loss: 0.5872 - val_accuracy: 0.7160 - lr: 1.2500e-04\n",
            "Epoch 24/25\n",
            "32/32 [==============================] - 10s 315ms/step - loss: 0.4702 - accuracy: 0.7510 - val_loss: 0.5863 - val_accuracy: 0.7360 - lr: 1.2500e-04\n",
            "Epoch 25/25\n",
            "32/32 [==============================] - 10s 312ms/step - loss: 0.5018 - accuracy: 0.7520 - val_loss: 0.5809 - val_accuracy: 0.7660 - lr: 1.2500e-04\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.5220 - accuracy: 0.7460\n",
            "Test accuracy with Data Augmentation, Dropout, Batch Normalization, and Learning Rate Scheduling: 0.7459999918937683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I developed and optimized a neural network from scratch for the Cats & Dogs classification task. I started with a simple CNN model and gradually introduced various techniques to improve performance and reduce overfitting. Here’s a summary of each approach and its impact on test accuracy:\n",
        "\n",
        "Baseline Model: The initial CNN model, trained with basic rescaling, achieved a test accuracy of 70.6%. This served as a starting point for further improvements.\n",
        "\n",
        "Data Augmentation: By adding data augmentation techniques such as rotation, width/height shift, shear, zoom, and horizontal flip, I aimed to increase data variability. The model achieved a test accuracy of 70.2%—a slight drop, but the model became more robust.\n",
        "\n",
        "Dropout: Introducing a 50% dropout layer after the dense layer helped reduce overfitting. With data augmentation and dropout, the model achieved a test accuracy of 70.8%, slightly improving stability.\n",
        "\n",
        "Batch Normalization: Adding batch normalization layers after each convolutional layer further stabilized learning. With data augmentation, dropout, and batch normalization, the test accuracy dropped to 60.6%, indicating potential issues with model complexity or over-regularization.\n",
        "\n",
        "Learning Rate Scheduling: Finally, I applied a learning rate scheduler (ReduceLROnPlateau) to adaptively decrease the learning rate based on validation loss. This setup achieved a test accuracy of 74.6%, the highest in this sequence, by allowing the model to fine-tune learning rates as training progressed.\n",
        "\n",
        "Overall, through incremental improvements—data augmentation, dropout, batch normalization, and learning rate scheduling—I was able to develop a more robust model, achieving the best performance with a combination of these techniques, culminating in a 74.6% test accuracy."
      ],
      "metadata": {
        "id": "bYEBRyNB4w1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 2\n",
        "# Imports\n",
        "import os\n",
        "import shutil\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Step 1: Increase the Training Sample Size (1200 images per class)\n",
        "\n",
        "# Define new training size (1200 images each for cats and dogs, 2400 in total)\n",
        "new_train_size = 1200\n",
        "\n",
        "# Set up paths\n",
        "base_dir = '/content/cats_and_dogs'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "source_dir = 'train'  # Assumes all images are in 'train_data' directory\n",
        "\n",
        "# Clear and recreate training directories if they exist\n",
        "if os.path.exists(train_dir):\n",
        "    shutil.rmtree(train_dir)\n",
        "os.makedirs(train_dir + '/cats', exist_ok=True)\n",
        "os.makedirs(train_dir + '/dogs', exist_ok=True)\n",
        "\n",
        "# Get lists of all images\n",
        "all_images = os.listdir(source_dir)\n",
        "cat_images = [img for img in all_images if 'cat' in img]\n",
        "dog_images = [img for img in all_images if 'dog' in img]\n",
        "\n",
        "# Function to copy images to the target directory\n",
        "def copy_images(images, start, end, target_dir):\n",
        "    for img in images[start:end]:\n",
        "        shutil.copy(os.path.join(source_dir, img), target_dir)\n",
        "\n",
        "# Copy 1200 images each for cats and dogs to the training directory\n",
        "copy_images(cat_images, 0, new_train_size, os.path.join(train_dir, 'cats'))\n",
        "copy_images(dog_images, 0, new_train_size, os.path.join(train_dir, 'dogs'))\n",
        "\n",
        "# Step 2: Define and Train the Baseline Model from Scratch\n",
        "\n",
        "# Basic data preprocessing (rescaling only, no augmentation yet)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Data generators for training, validation, and testing\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),      # Resize all images to 150x150 pixels\n",
        "    batch_size=32,               # Number of images to yield per batch\n",
        "    class_mode='binary'          # Binary classification (cats vs. dogs)\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Simple CNN Model (Baseline) without optimizations\n",
        "baseline_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the baseline model\n",
        "baseline_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the baseline model on the larger dataset (without optimizations)\n",
        "history_baseline = baseline_model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_generator),  # Total number of batches per epoch\n",
        "    epochs=25,                             # Use fewer epochs for the baseline test\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=len(validation_generator)  # Total number of validation batches per epoch\n",
        ")\n",
        "\n",
        "# Evaluate the baseline model on the test data\n",
        "test_loss_baseline, test_acc_baseline = baseline_model.evaluate(test_generator, steps=len(test_generator))\n",
        "print(f\"Test accuracy with increased training size (Baseline): {test_acc_baseline}\")\n",
        "\n",
        "# Step 3: Add Optimizations - Data Augmentation, Dropout, Batch Normalization\n",
        "\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
        "\n",
        "# Data Augmentation for the training set to prevent overfitting\n",
        "train_datagen_augmented = ImageDataGenerator(\n",
        "    rescale=1./255,             # Rescale pixel values from 0-255 to 0-1\n",
        "    rotation_range=40,           # Randomly rotate images up to 40 degrees\n",
        "    width_shift_range=0.2,       # Randomly shift images horizontally by 20%\n",
        "    height_shift_range=0.2,      # Randomly shift images vertically by 20%\n",
        "    shear_range=0.2,             # Apply shearing transformation\n",
        "    zoom_range=0.2,              # Randomly zoom in on images by 20%\n",
        "    horizontal_flip=True,        # Randomly flip images horizontally\n",
        "    fill_mode='nearest'          # Fill in missing pixels with the nearest ones\n",
        ")\n",
        "\n",
        "# Load augmented data for training\n",
        "train_generator_augmented = train_datagen_augmented.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Define the optimized model with Dropout and Batch Normalization\n",
        "optimized_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),                # Dropout layer to reduce overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the optimized model\n",
        "optimized_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the optimized model with augmented data\n",
        "history_optimized = optimized_model.fit(\n",
        "    train_generator_augmented,\n",
        "    steps_per_epoch=len(train_generator_augmented),\n",
        "    epochs=25,                   # More epochs to fully utilize the optimizations\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=len(validation_generator)\n",
        ")\n",
        "\n",
        "# Evaluate the optimized model on the test data\n",
        "test_loss_optimized, test_acc_optimized = optimized_model.evaluate(test_generator, steps=len(test_generator))\n",
        "print(f\"Test accuracy with Data Augmentation, Dropout, and Batch Normalization: {test_acc_optimized}\")\n"
      ],
      "metadata": {
        "id": "moYj3zW945Cr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "200f2eac-41ca-4426-902c-845d6378a255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2400 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "75/75 [==============================] - 18s 222ms/step - loss: 0.7599 - accuracy: 0.5108 - val_loss: 0.6964 - val_accuracy: 0.4980\n",
            "Epoch 2/25\n",
            "75/75 [==============================] - 16s 215ms/step - loss: 0.6778 - accuracy: 0.5767 - val_loss: 0.6385 - val_accuracy: 0.6360\n",
            "Epoch 3/25\n",
            "75/75 [==============================] - 16s 214ms/step - loss: 0.6317 - accuracy: 0.6558 - val_loss: 0.5621 - val_accuracy: 0.7320\n",
            "Epoch 4/25\n",
            "75/75 [==============================] - 16s 213ms/step - loss: 0.5517 - accuracy: 0.7175 - val_loss: 0.4531 - val_accuracy: 0.8020\n",
            "Epoch 5/25\n",
            "75/75 [==============================] - 16s 210ms/step - loss: 0.4566 - accuracy: 0.7921 - val_loss: 0.4020 - val_accuracy: 0.7960\n",
            "Epoch 6/25\n",
            "75/75 [==============================] - 16s 213ms/step - loss: 0.3512 - accuracy: 0.8429 - val_loss: 0.2472 - val_accuracy: 0.9080\n",
            "Epoch 7/25\n",
            "75/75 [==============================] - 16s 216ms/step - loss: 0.2645 - accuracy: 0.8913 - val_loss: 0.1519 - val_accuracy: 0.9540\n",
            "Epoch 8/25\n",
            "75/75 [==============================] - 15s 206ms/step - loss: 0.1671 - accuracy: 0.9354 - val_loss: 0.1027 - val_accuracy: 0.9640\n",
            "Epoch 9/25\n",
            "75/75 [==============================] - 16s 209ms/step - loss: 0.0980 - accuracy: 0.9696 - val_loss: 0.0545 - val_accuracy: 0.9820\n",
            "Epoch 10/25\n",
            "75/75 [==============================] - 16s 210ms/step - loss: 0.0546 - accuracy: 0.9812 - val_loss: 0.0702 - val_accuracy: 0.9720\n",
            "Epoch 11/25\n",
            "75/75 [==============================] - 16s 214ms/step - loss: 0.0417 - accuracy: 0.9879 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "Epoch 12/25\n",
            "75/75 [==============================] - 16s 211ms/step - loss: 0.0213 - accuracy: 0.9950 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
            "Epoch 13/25\n",
            "75/75 [==============================] - 16s 212ms/step - loss: 0.0098 - accuracy: 0.9992 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "Epoch 14/25\n",
            "75/75 [==============================] - 16s 212ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
            "Epoch 15/25\n",
            "75/75 [==============================] - 16s 212ms/step - loss: 6.3514e-04 - accuracy: 1.0000 - val_loss: 4.8209e-04 - val_accuracy: 1.0000\n",
            "Epoch 16/25\n",
            "75/75 [==============================] - 16s 215ms/step - loss: 3.7620e-04 - accuracy: 1.0000 - val_loss: 3.7838e-04 - val_accuracy: 1.0000\n",
            "Epoch 17/25\n",
            "75/75 [==============================] - 16s 213ms/step - loss: 2.7556e-04 - accuracy: 1.0000 - val_loss: 2.7653e-04 - val_accuracy: 1.0000\n",
            "Epoch 18/25\n",
            "75/75 [==============================] - 16s 213ms/step - loss: 2.1916e-04 - accuracy: 1.0000 - val_loss: 2.1034e-04 - val_accuracy: 1.0000\n",
            "Epoch 19/25\n",
            "75/75 [==============================] - 16s 216ms/step - loss: 1.7752e-04 - accuracy: 1.0000 - val_loss: 1.7570e-04 - val_accuracy: 1.0000\n",
            "Epoch 20/25\n",
            "75/75 [==============================] - 16s 215ms/step - loss: 1.4713e-04 - accuracy: 1.0000 - val_loss: 1.4936e-04 - val_accuracy: 1.0000\n",
            "Epoch 21/25\n",
            "75/75 [==============================] - 16s 213ms/step - loss: 1.2577e-04 - accuracy: 1.0000 - val_loss: 1.2543e-04 - val_accuracy: 1.0000\n",
            "Epoch 22/25\n",
            "75/75 [==============================] - 16s 210ms/step - loss: 1.0859e-04 - accuracy: 1.0000 - val_loss: 1.0859e-04 - val_accuracy: 1.0000\n",
            "Epoch 23/25\n",
            "75/75 [==============================] - 16s 214ms/step - loss: 9.4773e-05 - accuracy: 1.0000 - val_loss: 9.5183e-05 - val_accuracy: 1.0000\n",
            "Epoch 24/25\n",
            "75/75 [==============================] - 16s 214ms/step - loss: 8.3618e-05 - accuracy: 1.0000 - val_loss: 8.6063e-05 - val_accuracy: 1.0000\n",
            "Epoch 25/25\n",
            "75/75 [==============================] - 16s 212ms/step - loss: 7.4264e-05 - accuracy: 1.0000 - val_loss: 7.5767e-05 - val_accuracy: 1.0000\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 6.6709e-05 - accuracy: 1.0000\n",
            "Test accuracy with increased training size (Baseline): 1.0\n",
            "Found 2400 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "75/75 [==============================] - 24s 306ms/step - loss: 6.9689 - accuracy: 0.5550 - val_loss: 2.3745 - val_accuracy: 0.5600\n",
            "Epoch 2/25\n",
            "75/75 [==============================] - 23s 302ms/step - loss: 1.3698 - accuracy: 0.5683 - val_loss: 2.6955 - val_accuracy: 0.4940\n",
            "Epoch 3/25\n",
            "75/75 [==============================] - 23s 302ms/step - loss: 0.7001 - accuracy: 0.6025 - val_loss: 1.7395 - val_accuracy: 0.5280\n",
            "Epoch 4/25\n",
            "75/75 [==============================] - 22s 297ms/step - loss: 0.6510 - accuracy: 0.6392 - val_loss: 0.7441 - val_accuracy: 0.5920\n",
            "Epoch 5/25\n",
            "75/75 [==============================] - 23s 300ms/step - loss: 0.6378 - accuracy: 0.6392 - val_loss: 0.6750 - val_accuracy: 0.5740\n",
            "Epoch 6/25\n",
            "75/75 [==============================] - 22s 297ms/step - loss: 0.6234 - accuracy: 0.6417 - val_loss: 0.6841 - val_accuracy: 0.6200\n",
            "Epoch 7/25\n",
            "75/75 [==============================] - 22s 292ms/step - loss: 0.6355 - accuracy: 0.6475 - val_loss: 0.7179 - val_accuracy: 0.5700\n",
            "Epoch 8/25\n",
            "75/75 [==============================] - 22s 297ms/step - loss: 0.6364 - accuracy: 0.6408 - val_loss: 0.6428 - val_accuracy: 0.6360\n",
            "Epoch 9/25\n",
            "75/75 [==============================] - 22s 296ms/step - loss: 0.6390 - accuracy: 0.6467 - val_loss: 0.7286 - val_accuracy: 0.6420\n",
            "Epoch 10/25\n",
            "75/75 [==============================] - 22s 295ms/step - loss: 0.6445 - accuracy: 0.6475 - val_loss: 0.7555 - val_accuracy: 0.6160\n",
            "Epoch 11/25\n",
            "75/75 [==============================] - 22s 294ms/step - loss: 0.6271 - accuracy: 0.6517 - val_loss: 0.5910 - val_accuracy: 0.7020\n",
            "Epoch 12/25\n",
            "75/75 [==============================] - 22s 293ms/step - loss: 0.6196 - accuracy: 0.6575 - val_loss: 0.5765 - val_accuracy: 0.6920\n",
            "Epoch 13/25\n",
            "75/75 [==============================] - 22s 296ms/step - loss: 0.6201 - accuracy: 0.6637 - val_loss: 0.5934 - val_accuracy: 0.7000\n",
            "Epoch 14/25\n",
            "75/75 [==============================] - 23s 299ms/step - loss: 0.5957 - accuracy: 0.6821 - val_loss: 0.5681 - val_accuracy: 0.6960\n",
            "Epoch 15/25\n",
            "75/75 [==============================] - 22s 292ms/step - loss: 0.6062 - accuracy: 0.6792 - val_loss: 0.6094 - val_accuracy: 0.6860\n",
            "Epoch 16/25\n",
            "75/75 [==============================] - 22s 297ms/step - loss: 0.5975 - accuracy: 0.6771 - val_loss: 0.8076 - val_accuracy: 0.6340\n",
            "Epoch 17/25\n",
            "75/75 [==============================] - 22s 297ms/step - loss: 0.6018 - accuracy: 0.6754 - val_loss: 0.7511 - val_accuracy: 0.6660\n",
            "Epoch 18/25\n",
            "75/75 [==============================] - 22s 296ms/step - loss: 0.5899 - accuracy: 0.6837 - val_loss: 0.8433 - val_accuracy: 0.6300\n",
            "Epoch 19/25\n",
            "75/75 [==============================] - 23s 298ms/step - loss: 0.6259 - accuracy: 0.6621 - val_loss: 0.6094 - val_accuracy: 0.6580\n",
            "Epoch 20/25\n",
            "75/75 [==============================] - 22s 297ms/step - loss: 0.6144 - accuracy: 0.6683 - val_loss: 1.0088 - val_accuracy: 0.5460\n",
            "Epoch 21/25\n",
            "75/75 [==============================] - 22s 295ms/step - loss: 0.6089 - accuracy: 0.6796 - val_loss: 0.5862 - val_accuracy: 0.6740\n",
            "Epoch 22/25\n",
            "75/75 [==============================] - 22s 295ms/step - loss: 0.5995 - accuracy: 0.6746 - val_loss: 0.5878 - val_accuracy: 0.7360\n",
            "Epoch 23/25\n",
            "75/75 [==============================] - 22s 294ms/step - loss: 0.5969 - accuracy: 0.6858 - val_loss: 0.7679 - val_accuracy: 0.6480\n",
            "Epoch 24/25\n",
            "75/75 [==============================] - 23s 298ms/step - loss: 0.6037 - accuracy: 0.6758 - val_loss: 0.5485 - val_accuracy: 0.7180\n",
            "Epoch 25/25\n",
            "75/75 [==============================] - 22s 294ms/step - loss: 0.6168 - accuracy: 0.6771 - val_loss: 0.5319 - val_accuracy: 0.7380\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.5337 - accuracy: 0.7620\n",
            "Test accuracy with Data Augmentation, Dropout, and Batch Normalization: 0.7620000243186951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, I increased the training sample size to 1200 images per class (2400 total) and retrained a CNN model from scratch to improve performance on the Cats & Dogs classification task. Here’s a summary of each approach and the resulting performance:\n",
        "\n",
        "Baseline Model: With a larger training dataset (2400 images) and without any optimizations, the CNN model achieved a perfect test accuracy of 100%, indicating the potential for overfitting due to the increased sample size.\n",
        "\n",
        "Optimized Model: I introduced data augmentation techniques (rotation, shift, shear, zoom, and horizontal flip) to increase variability and prevent overfitting. Additionally, I added dropout and batch normalization layers to improve generalization and training stability. This optimized model achieved a test accuracy of 76.2%.\n",
        "\n",
        "By increasing the sample size and implementing data augmentation, dropout, and batch normalization, I successfully improved the model's robustness and balanced its generalization, albeit with some reduction in accuracy due to reduced overfitting tendencies. The final optimized model yielded a realistic performance of 76.2% test accuracy, reflecting a more generalizable approach compared to the overfitted baseline."
      ],
      "metadata": {
        "id": "2h8LMoza5ABW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question - 3\n",
        "import os\n",
        "import shutil\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Helper function to prepare the dataset with a specified training sample size\n",
        "def prepare_dataset(train_size_per_class):\n",
        "    \"\"\"\n",
        "    This function prepares the dataset by adjusting the number of training images per class\n",
        "    to the specified size, while keeping validation and test sets the same.\n",
        "    \"\"\"\n",
        "    # Set up paths\n",
        "    base_dir = '/content/cats_and_dogs'\n",
        "    train_dir = os.path.join(base_dir, 'train')\n",
        "    validation_dir = os.path.join(base_dir, 'validation')\n",
        "    test_dir = os.path.join(base_dir, 'test')\n",
        "    source_dir = 'train'  # Directory with all images\n",
        "\n",
        "    # Clear and recreate training directories if they exist\n",
        "    if os.path.exists(train_dir):\n",
        "        shutil.rmtree(train_dir)\n",
        "    os.makedirs(train_dir + '/cats', exist_ok=True)\n",
        "    os.makedirs(train_dir + '/dogs', exist_ok=True)\n",
        "\n",
        "    # Get lists of all images\n",
        "    all_images = os.listdir(source_dir)\n",
        "    cat_images = [img for img in all_images if 'cat' in img]\n",
        "    dog_images = [img for img in all_images if 'dog' in img]\n",
        "\n",
        "    # Function to copy images to the target directory\n",
        "    def copy_images(images, start, end, target_dir):\n",
        "        for img in images[start:end]:\n",
        "            shutil.copy(os.path.join(source_dir, img), target_dir)\n",
        "\n",
        "    # Copy images for specified training size per class\n",
        "    copy_images(cat_images, 0, train_size_per_class, os.path.join(train_dir, 'cats'))\n",
        "    copy_images(dog_images, 0, train_size_per_class, os.path.join(train_dir, 'dogs'))\n",
        "\n",
        "# Helper function to define, train, and evaluate the model\n",
        "def train_and_evaluate_model(train_size_per_class):\n",
        "    \"\"\"\n",
        "    This function defines the CNN model with data augmentation, dropout, and batch normalization,\n",
        "    trains it with the specified training sample size, and evaluates it on the test set.\n",
        "    \"\"\"\n",
        "    # Prepare the dataset for the specified training sample size\n",
        "    prepare_dataset(train_size_per_class)\n",
        "\n",
        "    # Data Augmentation for training set\n",
        "    train_datagen_augmented = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Rescale only for validation and test sets\n",
        "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    # Data Generators\n",
        "    train_generator_augmented = train_datagen_augmented.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    # Define the CNN Model with Dropout and Batch Normalization\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_generator_augmented,\n",
        "        steps_per_epoch=len(train_generator_augmented),\n",
        "        epochs=25,  # More epochs to fully utilize each dataset size\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=len(validation_generator)\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on test data\n",
        "    test_loss, test_acc = model.evaluate(test_generator, steps=len(test_generator))\n",
        "    print(f\"Test accuracy with training size ({train_size_per_class * 2} total images): {test_acc}\")\n",
        "    return test_acc\n",
        "\n",
        "# Experiment with different training sizes and record the results\n",
        "sample_sizes = [300, 600, 1500, 3000]  # Experiment with sizes smaller and larger than before\n",
        "results = {}\n",
        "\n",
        "for size in sample_sizes:\n",
        "    print(f\"\\nTraining with sample size: {size * 2} (Total images: {size * 2})\")\n",
        "    results[size * 2] = train_and_evaluate_model(size)\n",
        "\n",
        "# Display the results for comparison\n",
        "print(\"\\nResults for Different Training Sizes:\")\n",
        "for size, accuracy in results.items():\n",
        "    print(f\"Training Size: {size} images, Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "MdIpo05k5DOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae24623a-7267-420a-ad0d-946e63089193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with sample size: 600 (Total images: 600)\n",
            "Found 600 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "19/19 [==============================] - 8s 362ms/step - loss: 7.9478 - accuracy: 0.5467 - val_loss: 4.6508 - val_accuracy: 0.5000\n",
            "Epoch 2/25\n",
            "19/19 [==============================] - 7s 343ms/step - loss: 3.6523 - accuracy: 0.5717 - val_loss: 4.0907 - val_accuracy: 0.5280\n",
            "Epoch 3/25\n",
            "19/19 [==============================] - 7s 345ms/step - loss: 2.1038 - accuracy: 0.5717 - val_loss: 6.6496 - val_accuracy: 0.5000\n",
            "Epoch 4/25\n",
            "19/19 [==============================] - 7s 341ms/step - loss: 1.2115 - accuracy: 0.6317 - val_loss: 13.3004 - val_accuracy: 0.5000\n",
            "Epoch 5/25\n",
            "19/19 [==============================] - 7s 341ms/step - loss: 0.8980 - accuracy: 0.6300 - val_loss: 8.7373 - val_accuracy: 0.5020\n",
            "Epoch 6/25\n",
            "19/19 [==============================] - 7s 344ms/step - loss: 0.8198 - accuracy: 0.6033 - val_loss: 8.8132 - val_accuracy: 0.5100\n",
            "Epoch 7/25\n",
            "19/19 [==============================] - 7s 338ms/step - loss: 0.7043 - accuracy: 0.6233 - val_loss: 8.1600 - val_accuracy: 0.5060\n",
            "Epoch 8/25\n",
            "19/19 [==============================] - 7s 342ms/step - loss: 0.6590 - accuracy: 0.6250 - val_loss: 6.3779 - val_accuracy: 0.5100\n",
            "Epoch 9/25\n",
            "19/19 [==============================] - 7s 337ms/step - loss: 0.6397 - accuracy: 0.6500 - val_loss: 7.5792 - val_accuracy: 0.5120\n",
            "Epoch 10/25\n",
            "19/19 [==============================] - 7s 342ms/step - loss: 0.6425 - accuracy: 0.6417 - val_loss: 7.6406 - val_accuracy: 0.5020\n",
            "Epoch 11/25\n",
            "19/19 [==============================] - 7s 344ms/step - loss: 0.6384 - accuracy: 0.6517 - val_loss: 7.7665 - val_accuracy: 0.5000\n",
            "Epoch 12/25\n",
            "19/19 [==============================] - 7s 338ms/step - loss: 0.6394 - accuracy: 0.6600 - val_loss: 4.2273 - val_accuracy: 0.5180\n",
            "Epoch 13/25\n",
            "19/19 [==============================] - 7s 340ms/step - loss: 0.6123 - accuracy: 0.6650 - val_loss: 3.3091 - val_accuracy: 0.5280\n",
            "Epoch 14/25\n",
            "19/19 [==============================] - 7s 340ms/step - loss: 0.6422 - accuracy: 0.6667 - val_loss: 2.7892 - val_accuracy: 0.5240\n",
            "Epoch 15/25\n",
            "19/19 [==============================] - 7s 340ms/step - loss: 0.6072 - accuracy: 0.6417 - val_loss: 4.1935 - val_accuracy: 0.5020\n",
            "Epoch 16/25\n",
            "19/19 [==============================] - 7s 339ms/step - loss: 0.6320 - accuracy: 0.6367 - val_loss: 2.1631 - val_accuracy: 0.5680\n",
            "Epoch 17/25\n",
            "19/19 [==============================] - 6s 335ms/step - loss: 0.6235 - accuracy: 0.6650 - val_loss: 1.1761 - val_accuracy: 0.5560\n",
            "Epoch 18/25\n",
            "19/19 [==============================] - 7s 336ms/step - loss: 0.6155 - accuracy: 0.6633 - val_loss: 2.1948 - val_accuracy: 0.5340\n",
            "Epoch 19/25\n",
            "19/19 [==============================] - 7s 339ms/step - loss: 0.6383 - accuracy: 0.6750 - val_loss: 1.5278 - val_accuracy: 0.5840\n",
            "Epoch 20/25\n",
            "19/19 [==============================] - 6s 333ms/step - loss: 0.5950 - accuracy: 0.6700 - val_loss: 1.2280 - val_accuracy: 0.5900\n",
            "Epoch 21/25\n",
            "19/19 [==============================] - 6s 335ms/step - loss: 0.5871 - accuracy: 0.6600 - val_loss: 1.4909 - val_accuracy: 0.5800\n",
            "Epoch 22/25\n",
            "19/19 [==============================] - 6s 335ms/step - loss: 0.6134 - accuracy: 0.6817 - val_loss: 2.0379 - val_accuracy: 0.6400\n",
            "Epoch 23/25\n",
            "19/19 [==============================] - 7s 336ms/step - loss: 0.5900 - accuracy: 0.6933 - val_loss: 0.8818 - val_accuracy: 0.6140\n",
            "Epoch 24/25\n",
            "19/19 [==============================] - 7s 337ms/step - loss: 0.6261 - accuracy: 0.6483 - val_loss: 1.0031 - val_accuracy: 0.6460\n",
            "Epoch 25/25\n",
            "19/19 [==============================] - 7s 337ms/step - loss: 0.6417 - accuracy: 0.6700 - val_loss: 1.5033 - val_accuracy: 0.6160\n",
            "16/16 [==============================] - 1s 68ms/step - loss: 1.0465 - accuracy: 0.6800\n",
            "Test accuracy with training size (600 total images): 0.6800000071525574\n",
            "\n",
            "Training with sample size: 1200 (Total images: 1200)\n",
            "Found 1200 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "38/38 [==============================] - 14s 323ms/step - loss: 7.8244 - accuracy: 0.5400 - val_loss: 2.8237 - val_accuracy: 0.4960\n",
            "Epoch 2/25\n",
            "38/38 [==============================] - 12s 306ms/step - loss: 3.2630 - accuracy: 0.5650 - val_loss: 1.5515 - val_accuracy: 0.5920\n",
            "Epoch 3/25\n",
            "38/38 [==============================] - 12s 308ms/step - loss: 0.8790 - accuracy: 0.5683 - val_loss: 6.9075 - val_accuracy: 0.4980\n",
            "Epoch 4/25\n",
            "38/38 [==============================] - 12s 311ms/step - loss: 0.8316 - accuracy: 0.5892 - val_loss: 4.8801 - val_accuracy: 0.5120\n",
            "Epoch 5/25\n",
            "38/38 [==============================] - 12s 305ms/step - loss: 0.7017 - accuracy: 0.6025 - val_loss: 3.2315 - val_accuracy: 0.4980\n",
            "Epoch 6/25\n",
            "38/38 [==============================] - 12s 313ms/step - loss: 0.6774 - accuracy: 0.6200 - val_loss: 2.5672 - val_accuracy: 0.5100\n",
            "Epoch 7/25\n",
            "38/38 [==============================] - 12s 309ms/step - loss: 0.6581 - accuracy: 0.6392 - val_loss: 1.7584 - val_accuracy: 0.5280\n",
            "Epoch 8/25\n",
            "38/38 [==============================] - 12s 309ms/step - loss: 0.6376 - accuracy: 0.6375 - val_loss: 1.4837 - val_accuracy: 0.5140\n",
            "Epoch 9/25\n",
            "38/38 [==============================] - 12s 310ms/step - loss: 0.6273 - accuracy: 0.6358 - val_loss: 0.9520 - val_accuracy: 0.5820\n",
            "Epoch 10/25\n",
            "38/38 [==============================] - 12s 309ms/step - loss: 0.6565 - accuracy: 0.6342 - val_loss: 1.1234 - val_accuracy: 0.5700\n",
            "Epoch 11/25\n",
            "38/38 [==============================] - 12s 308ms/step - loss: 0.6587 - accuracy: 0.6358 - val_loss: 0.6932 - val_accuracy: 0.6400\n",
            "Epoch 12/25\n",
            "38/38 [==============================] - 12s 310ms/step - loss: 0.6212 - accuracy: 0.6658 - val_loss: 0.7594 - val_accuracy: 0.6420\n",
            "Epoch 13/25\n",
            "38/38 [==============================] - 12s 307ms/step - loss: 0.6334 - accuracy: 0.6467 - val_loss: 0.6740 - val_accuracy: 0.6580\n",
            "Epoch 14/25\n",
            "38/38 [==============================] - 12s 307ms/step - loss: 0.6288 - accuracy: 0.6425 - val_loss: 0.6339 - val_accuracy: 0.6480\n",
            "Epoch 15/25\n",
            "38/38 [==============================] - 12s 308ms/step - loss: 0.6314 - accuracy: 0.6692 - val_loss: 0.6140 - val_accuracy: 0.6820\n",
            "Epoch 16/25\n",
            "38/38 [==============================] - 12s 310ms/step - loss: 0.6053 - accuracy: 0.6650 - val_loss: 0.6603 - val_accuracy: 0.6740\n",
            "Epoch 17/25\n",
            "38/38 [==============================] - 12s 309ms/step - loss: 0.6410 - accuracy: 0.6425 - val_loss: 0.6454 - val_accuracy: 0.6800\n",
            "Epoch 18/25\n",
            "38/38 [==============================] - 12s 310ms/step - loss: 0.6077 - accuracy: 0.6617 - val_loss: 0.6312 - val_accuracy: 0.6860\n",
            "Epoch 19/25\n",
            "38/38 [==============================] - 12s 310ms/step - loss: 0.6085 - accuracy: 0.6700 - val_loss: 1.3194 - val_accuracy: 0.5860\n",
            "Epoch 20/25\n",
            "38/38 [==============================] - 12s 306ms/step - loss: 0.6634 - accuracy: 0.6533 - val_loss: 1.3566 - val_accuracy: 0.5680\n",
            "Epoch 21/25\n",
            "38/38 [==============================] - 12s 305ms/step - loss: 0.6075 - accuracy: 0.6717 - val_loss: 0.6239 - val_accuracy: 0.6360\n",
            "Epoch 22/25\n",
            "38/38 [==============================] - 12s 306ms/step - loss: 0.6008 - accuracy: 0.6833 - val_loss: 0.5767 - val_accuracy: 0.6900\n",
            "Epoch 23/25\n",
            "38/38 [==============================] - 12s 310ms/step - loss: 0.5911 - accuracy: 0.6842 - val_loss: 0.7061 - val_accuracy: 0.5480\n",
            "Epoch 24/25\n",
            "38/38 [==============================] - 12s 307ms/step - loss: 0.5973 - accuracy: 0.6800 - val_loss: 0.5513 - val_accuracy: 0.7240\n",
            "Epoch 25/25\n",
            "38/38 [==============================] - 12s 312ms/step - loss: 0.5763 - accuracy: 0.7250 - val_loss: 0.5613 - val_accuracy: 0.7120\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.5610 - accuracy: 0.6980\n",
            "Test accuracy with training size (1200 total images): 0.6980000138282776\n",
            "\n",
            "Training with sample size: 3000 (Total images: 3000)\n",
            "Found 3000 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "94/94 [==============================] - 30s 302ms/step - loss: 5.0423 - accuracy: 0.5460 - val_loss: 2.6046 - val_accuracy: 0.5160\n",
            "Epoch 2/25\n",
            "94/94 [==============================] - 28s 298ms/step - loss: 0.7554 - accuracy: 0.5797 - val_loss: 1.0755 - val_accuracy: 0.5300\n",
            "Epoch 3/25\n",
            "94/94 [==============================] - 28s 294ms/step - loss: 0.6744 - accuracy: 0.6073 - val_loss: 0.7970 - val_accuracy: 0.5640\n",
            "Epoch 4/25\n",
            "94/94 [==============================] - 28s 292ms/step - loss: 0.6680 - accuracy: 0.6187 - val_loss: 0.7818 - val_accuracy: 0.5540\n",
            "Epoch 5/25\n",
            "94/94 [==============================] - 28s 298ms/step - loss: 0.6457 - accuracy: 0.6347 - val_loss: 0.6460 - val_accuracy: 0.5900\n",
            "Epoch 6/25\n",
            "94/94 [==============================] - 28s 294ms/step - loss: 0.6435 - accuracy: 0.6307 - val_loss: 0.6583 - val_accuracy: 0.5820\n",
            "Epoch 7/25\n",
            "94/94 [==============================] - 28s 294ms/step - loss: 0.6342 - accuracy: 0.6433 - val_loss: 0.5897 - val_accuracy: 0.6800\n",
            "Epoch 8/25\n",
            "94/94 [==============================] - 28s 294ms/step - loss: 0.6293 - accuracy: 0.6387 - val_loss: 0.7698 - val_accuracy: 0.6120\n",
            "Epoch 9/25\n",
            "94/94 [==============================] - 28s 291ms/step - loss: 0.6513 - accuracy: 0.6313 - val_loss: 0.6634 - val_accuracy: 0.6260\n",
            "Epoch 10/25\n",
            "94/94 [==============================] - 28s 293ms/step - loss: 0.6435 - accuracy: 0.6330 - val_loss: 0.8948 - val_accuracy: 0.6060\n",
            "Epoch 11/25\n",
            "94/94 [==============================] - 28s 294ms/step - loss: 0.7156 - accuracy: 0.6327 - val_loss: 0.7361 - val_accuracy: 0.5520\n",
            "Epoch 12/25\n",
            "94/94 [==============================] - 28s 291ms/step - loss: 0.6463 - accuracy: 0.6417 - val_loss: 0.5979 - val_accuracy: 0.6600\n",
            "Epoch 13/25\n",
            "94/94 [==============================] - 28s 292ms/step - loss: 0.6215 - accuracy: 0.6497 - val_loss: 0.5598 - val_accuracy: 0.7200\n",
            "Epoch 14/25\n",
            "94/94 [==============================] - 28s 295ms/step - loss: 0.6219 - accuracy: 0.6523 - val_loss: 0.5959 - val_accuracy: 0.6680\n",
            "Epoch 15/25\n",
            "94/94 [==============================] - 27s 290ms/step - loss: 0.6271 - accuracy: 0.6530 - val_loss: 0.7227 - val_accuracy: 0.6280\n",
            "Epoch 16/25\n",
            "94/94 [==============================] - 27s 289ms/step - loss: 0.6001 - accuracy: 0.6670 - val_loss: 0.6241 - val_accuracy: 0.7100\n",
            "Epoch 17/25\n",
            "94/94 [==============================] - 28s 293ms/step - loss: 0.6131 - accuracy: 0.6710 - val_loss: 0.5705 - val_accuracy: 0.6880\n",
            "Epoch 18/25\n",
            "94/94 [==============================] - 28s 293ms/step - loss: 0.6037 - accuracy: 0.6670 - val_loss: 0.5817 - val_accuracy: 0.7060\n",
            "Epoch 19/25\n",
            "94/94 [==============================] - 28s 291ms/step - loss: 0.5909 - accuracy: 0.6850 - val_loss: 0.5618 - val_accuracy: 0.7340\n",
            "Epoch 20/25\n",
            "94/94 [==============================] - 28s 293ms/step - loss: 0.5850 - accuracy: 0.6847 - val_loss: 0.7384 - val_accuracy: 0.6540\n",
            "Epoch 21/25\n",
            "94/94 [==============================] - 27s 289ms/step - loss: 0.5929 - accuracy: 0.6777 - val_loss: 0.5577 - val_accuracy: 0.6680\n",
            "Epoch 22/25\n",
            "94/94 [==============================] - 28s 291ms/step - loss: 0.6056 - accuracy: 0.6850 - val_loss: 0.5942 - val_accuracy: 0.6940\n",
            "Epoch 23/25\n",
            "94/94 [==============================] - 27s 289ms/step - loss: 0.5964 - accuracy: 0.6797 - val_loss: 0.5409 - val_accuracy: 0.7480\n",
            "Epoch 24/25\n",
            "94/94 [==============================] - 28s 292ms/step - loss: 0.5874 - accuracy: 0.6847 - val_loss: 0.6422 - val_accuracy: 0.5480\n",
            "Epoch 25/25\n",
            "94/94 [==============================] - 28s 291ms/step - loss: 0.5785 - accuracy: 0.6860 - val_loss: 0.5459 - val_accuracy: 0.7600\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.5335 - accuracy: 0.7200\n",
            "Test accuracy with training size (3000 total images): 0.7200000286102295\n",
            "\n",
            "Training with sample size: 6000 (Total images: 6000)\n",
            "Found 6000 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "188/188 [==============================] - 57s 291ms/step - loss: 2.3393 - accuracy: 0.5560 - val_loss: 1.0921 - val_accuracy: 0.5060\n",
            "Epoch 2/25\n",
            "188/188 [==============================] - 54s 286ms/step - loss: 0.6619 - accuracy: 0.6057 - val_loss: 0.6965 - val_accuracy: 0.5780\n",
            "Epoch 3/25\n",
            "188/188 [==============================] - 54s 286ms/step - loss: 0.6641 - accuracy: 0.6075 - val_loss: 0.8413 - val_accuracy: 0.6040\n",
            "Epoch 4/25\n",
            "188/188 [==============================] - 54s 284ms/step - loss: 0.6491 - accuracy: 0.6260 - val_loss: 0.6830 - val_accuracy: 0.5760\n",
            "Epoch 5/25\n",
            "188/188 [==============================] - 54s 287ms/step - loss: 0.6327 - accuracy: 0.6373 - val_loss: 0.5990 - val_accuracy: 0.6880\n",
            "Epoch 6/25\n",
            "188/188 [==============================] - 54s 288ms/step - loss: 0.6289 - accuracy: 0.6405 - val_loss: 0.6482 - val_accuracy: 0.6480\n",
            "Epoch 7/25\n",
            "188/188 [==============================] - 54s 287ms/step - loss: 0.6148 - accuracy: 0.6555 - val_loss: 1.0099 - val_accuracy: 0.5840\n",
            "Epoch 8/25\n",
            "188/188 [==============================] - 54s 288ms/step - loss: 0.6144 - accuracy: 0.6625 - val_loss: 0.5545 - val_accuracy: 0.7700\n",
            "Epoch 9/25\n",
            "188/188 [==============================] - 54s 287ms/step - loss: 0.6100 - accuracy: 0.6693 - val_loss: 0.6588 - val_accuracy: 0.6360\n",
            "Epoch 10/25\n",
            "188/188 [==============================] - 54s 288ms/step - loss: 0.5909 - accuracy: 0.6872 - val_loss: 0.6733 - val_accuracy: 0.6400\n",
            "Epoch 11/25\n",
            "188/188 [==============================] - 54s 289ms/step - loss: 0.5871 - accuracy: 0.6860 - val_loss: 0.5552 - val_accuracy: 0.7380\n",
            "Epoch 12/25\n",
            "188/188 [==============================] - 54s 287ms/step - loss: 0.5968 - accuracy: 0.6737 - val_loss: 0.7608 - val_accuracy: 0.6460\n",
            "Epoch 13/25\n",
            "188/188 [==============================] - 54s 288ms/step - loss: 0.5846 - accuracy: 0.6897 - val_loss: 0.7109 - val_accuracy: 0.6340\n",
            "Epoch 14/25\n",
            "188/188 [==============================] - 54s 289ms/step - loss: 0.5712 - accuracy: 0.7013 - val_loss: 0.4901 - val_accuracy: 0.7780\n",
            "Epoch 15/25\n",
            "188/188 [==============================] - 54s 286ms/step - loss: 0.5630 - accuracy: 0.7062 - val_loss: 1.1704 - val_accuracy: 0.6240\n",
            "Epoch 16/25\n",
            "188/188 [==============================] - 54s 288ms/step - loss: 0.5426 - accuracy: 0.7232 - val_loss: 0.5309 - val_accuracy: 0.7300\n",
            "Epoch 17/25\n",
            "188/188 [==============================] - 54s 287ms/step - loss: 0.5509 - accuracy: 0.7252 - val_loss: 0.5592 - val_accuracy: 0.7580\n",
            "Epoch 18/25\n",
            "188/188 [==============================] - 53s 283ms/step - loss: 0.5380 - accuracy: 0.7297 - val_loss: 0.9559 - val_accuracy: 0.5840\n",
            "Epoch 19/25\n",
            "188/188 [==============================] - 54s 287ms/step - loss: 0.5346 - accuracy: 0.7393 - val_loss: 0.8047 - val_accuracy: 0.6500\n",
            "Epoch 20/25\n",
            "188/188 [==============================] - 54s 286ms/step - loss: 0.5303 - accuracy: 0.7367 - val_loss: 0.5020 - val_accuracy: 0.7540\n",
            "Epoch 21/25\n",
            "188/188 [==============================] - 54s 285ms/step - loss: 0.5333 - accuracy: 0.7423 - val_loss: 0.5178 - val_accuracy: 0.7500\n",
            "Epoch 22/25\n",
            "188/188 [==============================] - 53s 283ms/step - loss: 0.5255 - accuracy: 0.7305 - val_loss: 1.7277 - val_accuracy: 0.5840\n",
            "Epoch 23/25\n",
            "188/188 [==============================] - 54s 285ms/step - loss: 0.5286 - accuracy: 0.7437 - val_loss: 0.6423 - val_accuracy: 0.7140\n",
            "Epoch 24/25\n",
            "188/188 [==============================] - 54s 285ms/step - loss: 0.5095 - accuracy: 0.7455 - val_loss: 0.4003 - val_accuracy: 0.8320\n",
            "Epoch 25/25\n",
            "188/188 [==============================] - 54s 286ms/step - loss: 0.4846 - accuracy: 0.7802 - val_loss: 0.4397 - val_accuracy: 0.7800\n",
            "16/16 [==============================] - 1s 67ms/step - loss: 0.4255 - accuracy: 0.8020\n",
            "Test accuracy with training size (6000 total images): 0.8019999861717224\n",
            "\n",
            "Results for Different Training Sizes:\n",
            "Training Size: 600 images, Test Accuracy: 0.6800000071525574\n",
            "Training Size: 1200 images, Test Accuracy: 0.6980000138282776\n",
            "Training Size: 3000 images, Test Accuracy: 0.7200000286102295\n",
            "Training Size: 6000 images, Test Accuracy: 0.8019999861717224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine the optimal training sample size for the Cats & Dogs classification task, I trained a model with increasing training sample sizes: 600, 1200, 3000, and 6000 images, while keeping validation and test sets constant. I applied data augmentation, dropout, and batch normalization in the model to improve performance and reduce overfitting.\n",
        "\n",
        "The test accuracies achieved with each training size were as follows:\n",
        "\n",
        "600 images: 68.0% test accuracy\n",
        "1200 images: 69.8% test accuracy\n",
        "3000 images: 72.0% test accuracy\n",
        "6000 images: 80.2% test accuracy\n",
        "With the largest sample size of 6000 images, the model achieved the best test accuracy of 80.2%, suggesting that a higher training sample size positively impacts model performance in this task."
      ],
      "metadata": {
        "id": "BA2igKP2RI-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Questiopn-04\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Helper function to prepare the dataset with a specified training sample size\n",
        "def prepare_dataset(train_size_per_class):\n",
        "    \"\"\"\n",
        "    This function prepares the dataset by adjusting the number of training images per class\n",
        "    to the specified size, while keeping validation and test sets the same.\n",
        "    \"\"\"\n",
        "    # Set up paths\n",
        "    base_dir = '/content/cats_and_dogs'\n",
        "    train_dir = os.path.join(base_dir, 'train')\n",
        "    validation_dir = os.path.join(base_dir, 'validation')\n",
        "    test_dir = os.path.join(base_dir, 'test')\n",
        "    source_dir = 'train'  # Directory with all images\n",
        "\n",
        "    # Clear and recreate training directories if they exist\n",
        "    if os.path.exists(train_dir):\n",
        "        shutil.rmtree(train_dir)\n",
        "    os.makedirs(train_dir + '/cats', exist_ok=True)\n",
        "    os.makedirs(train_dir + '/dogs', exist_ok=True)\n",
        "\n",
        "    # Get lists of all images\n",
        "    all_images = os.listdir(source_dir)\n",
        "    cat_images = [img for img in all_images if 'cat' in img]\n",
        "    dog_images = [img for img in all_images if 'dog' in img]\n",
        "\n",
        "    # Function to copy images to the target directory\n",
        "    def copy_images(images, start, end, target_dir):\n",
        "        for img in images[start:end]:\n",
        "            shutil.copy(os.path.join(source_dir, img), target_dir)\n",
        "\n",
        "    # Copy images for specified training size per class\n",
        "    copy_images(cat_images, 0, train_size_per_class, os.path.join(train_dir, 'cats'))\n",
        "    copy_images(dog_images, 0, train_size_per_class, os.path.join(train_dir, 'dogs'))\n",
        "\n",
        "# Helper function to define, train, and evaluate the model with a pretrained base\n",
        "def train_and_evaluate_model_with_pretrained(train_size_per_class):\n",
        "    \"\"\"\n",
        "    This function defines a CNN model with a pretrained VGG16 base, additional dense layers, and\n",
        "    optimizations. It trains the model with the specified training sample size and evaluates it on the test set.\n",
        "    \"\"\"\n",
        "    # Prepare the dataset for the specified training sample size\n",
        "    prepare_dataset(train_size_per_class)\n",
        "\n",
        "    # Data Augmentation for training set\n",
        "    train_datagen_augmented = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Rescale only for validation and test sets\n",
        "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    # Data Generators\n",
        "    train_generator_augmented = train_datagen_augmented.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    # Define the pretrained VGG16 base model\n",
        "    vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "\n",
        "    # Freeze the base model layers initially\n",
        "    vgg16_base.trainable = False\n",
        "\n",
        "    # Define the model architecture with the pretrained base and custom top layers\n",
        "    model = Sequential([\n",
        "        vgg16_base,\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model with a lower learning rate\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Add a learning rate scheduler\n",
        "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "    # Train the model with augmented data\n",
        "    history = model.fit(\n",
        "        train_generator_augmented,\n",
        "        steps_per_epoch=len(train_generator_augmented),\n",
        "        epochs=25,  # Use more epochs to fully utilize each dataset size\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=len(validation_generator),\n",
        "        callbacks=[lr_scheduler]\n",
        "    )\n",
        "\n",
        "    # Unfreeze the last few layers of the base model for fine-tuning\n",
        "    for layer in vgg16_base.layers[-4:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    # Recompile the model with a lower learning rate for fine-tuning\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Fine-tune the model\n",
        "    history_finetune = model.fit(\n",
        "        train_generator_augmented,\n",
        "        steps_per_epoch=len(train_generator_augmented),\n",
        "        epochs=25,  # Fine-tune for fewer epochs\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=len(validation_generator),\n",
        "        callbacks=[lr_scheduler]\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on test data\n",
        "    test_loss, test_acc = model.evaluate(test_generator, steps=len(test_generator))\n",
        "    print(f\"Test accuracy with pretrained model and training size ({train_size_per_class * 2} total images): {test_acc}\")\n",
        "    return test_acc\n",
        "\n",
        "# Experiment with different training sizes and record the results\n",
        "sample_sizes = [300, 600, 1500, 3000]  # Experiment with sizes smaller and larger than before\n",
        "results = {}\n",
        "\n",
        "for size in sample_sizes:\n",
        "    print(f\"\\nTraining with pretrained model and sample size: {size * 2} (Total images: {size * 2})\")\n",
        "    results[size * 2] = train_and_evaluate_model_with_pretrained(size)\n",
        "\n",
        "# Display the results for comparison\n",
        "print(\"\\nResults for Different Training Sizes (Pretrained Network):\")\n",
        "for size, accuracy in results.items():\n",
        "    print(f\"Training Size: {size} images, Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "3SGFb_lQPv8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9dcda1a-9166-48aa-e40e-cbb6af41370c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with pretrained model and sample size: 600 (Total images: 600)\n",
            "Found 600 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Epoch 1/25\n",
            "19/19 [==============================] - 13s 605ms/step - loss: 0.7506 - accuracy: 0.6433 - val_loss: 0.4955 - val_accuracy: 0.7520 - lr: 1.0000e-04\n",
            "Epoch 2/25\n",
            "19/19 [==============================] - 11s 562ms/step - loss: 0.4886 - accuracy: 0.7650 - val_loss: 0.4308 - val_accuracy: 0.7880 - lr: 1.0000e-04\n",
            "Epoch 3/25\n",
            "19/19 [==============================] - 11s 560ms/step - loss: 0.4846 - accuracy: 0.8017 - val_loss: 0.3744 - val_accuracy: 0.8420 - lr: 1.0000e-04\n",
            "Epoch 4/25\n",
            "19/19 [==============================] - 11s 568ms/step - loss: 0.3740 - accuracy: 0.8183 - val_loss: 0.3307 - val_accuracy: 0.8680 - lr: 1.0000e-04\n",
            "Epoch 5/25\n",
            "19/19 [==============================] - 11s 562ms/step - loss: 0.3542 - accuracy: 0.8467 - val_loss: 0.3397 - val_accuracy: 0.8460 - lr: 1.0000e-04\n",
            "Epoch 6/25\n",
            "19/19 [==============================] - 11s 563ms/step - loss: 0.3282 - accuracy: 0.8500 - val_loss: 0.3125 - val_accuracy: 0.8680 - lr: 1.0000e-04\n",
            "Epoch 7/25\n",
            "19/19 [==============================] - 11s 567ms/step - loss: 0.3165 - accuracy: 0.8550 - val_loss: 0.3377 - val_accuracy: 0.8520 - lr: 1.0000e-04\n",
            "Epoch 8/25\n",
            "19/19 [==============================] - 10s 556ms/step - loss: 0.3204 - accuracy: 0.8650 - val_loss: 0.3126 - val_accuracy: 0.8680 - lr: 1.0000e-04\n",
            "Epoch 9/25\n",
            "19/19 [==============================] - 11s 567ms/step - loss: 0.3108 - accuracy: 0.8750 - val_loss: 0.3049 - val_accuracy: 0.8640 - lr: 1.0000e-04\n",
            "Epoch 10/25\n",
            "19/19 [==============================] - 11s 568ms/step - loss: 0.2953 - accuracy: 0.8700 - val_loss: 0.3165 - val_accuracy: 0.8580 - lr: 1.0000e-04\n",
            "Epoch 11/25\n",
            "19/19 [==============================] - 11s 569ms/step - loss: 0.3267 - accuracy: 0.8683 - val_loss: 0.2911 - val_accuracy: 0.8780 - lr: 1.0000e-04\n",
            "Epoch 12/25\n",
            "19/19 [==============================] - 11s 561ms/step - loss: 0.3004 - accuracy: 0.8633 - val_loss: 0.2877 - val_accuracy: 0.8740 - lr: 1.0000e-04\n",
            "Epoch 13/25\n",
            "19/19 [==============================] - 11s 560ms/step - loss: 0.3076 - accuracy: 0.8700 - val_loss: 0.3384 - val_accuracy: 0.8440 - lr: 1.0000e-04\n",
            "Epoch 14/25\n",
            "19/19 [==============================] - 11s 563ms/step - loss: 0.3166 - accuracy: 0.8667 - val_loss: 0.3288 - val_accuracy: 0.8680 - lr: 1.0000e-04\n",
            "Epoch 15/25\n",
            "19/19 [==============================] - 11s 568ms/step - loss: 0.2446 - accuracy: 0.8850 - val_loss: 0.2951 - val_accuracy: 0.8800 - lr: 1.0000e-04\n",
            "Epoch 16/25\n",
            "19/19 [==============================] - 11s 560ms/step - loss: 0.2557 - accuracy: 0.8883 - val_loss: 0.3016 - val_accuracy: 0.8720 - lr: 5.0000e-05\n",
            "Epoch 17/25\n",
            "19/19 [==============================] - 11s 579ms/step - loss: 0.2313 - accuracy: 0.9017 - val_loss: 0.2938 - val_accuracy: 0.8740 - lr: 5.0000e-05\n",
            "Epoch 18/25\n",
            "19/19 [==============================] - 11s 569ms/step - loss: 0.2798 - accuracy: 0.8767 - val_loss: 0.2935 - val_accuracy: 0.8840 - lr: 5.0000e-05\n",
            "Epoch 19/25\n",
            "19/19 [==============================] - 10s 556ms/step - loss: 0.2658 - accuracy: 0.8767 - val_loss: 0.3021 - val_accuracy: 0.8740 - lr: 2.5000e-05\n",
            "Epoch 20/25\n",
            "19/19 [==============================] - 11s 563ms/step - loss: 0.3199 - accuracy: 0.8533 - val_loss: 0.3059 - val_accuracy: 0.8740 - lr: 2.5000e-05\n",
            "Epoch 21/25\n",
            "19/19 [==============================] - 11s 568ms/step - loss: 0.2593 - accuracy: 0.8867 - val_loss: 0.3161 - val_accuracy: 0.8740 - lr: 2.5000e-05\n",
            "Epoch 22/25\n",
            "19/19 [==============================] - 10s 546ms/step - loss: 0.3049 - accuracy: 0.8783 - val_loss: 0.3221 - val_accuracy: 0.8640 - lr: 1.2500e-05\n",
            "Epoch 23/25\n",
            "19/19 [==============================] - 11s 564ms/step - loss: 0.2653 - accuracy: 0.8850 - val_loss: 0.3221 - val_accuracy: 0.8720 - lr: 1.2500e-05\n",
            "Epoch 24/25\n",
            "19/19 [==============================] - 11s 567ms/step - loss: 0.2393 - accuracy: 0.9000 - val_loss: 0.3200 - val_accuracy: 0.8780 - lr: 1.2500e-05\n",
            "Epoch 25/25\n",
            "19/19 [==============================] - 11s 565ms/step - loss: 0.2325 - accuracy: 0.9050 - val_loss: 0.3198 - val_accuracy: 0.8760 - lr: 6.2500e-06\n",
            "Epoch 1/25\n",
            "19/19 [==============================] - 12s 594ms/step - loss: 0.2548 - accuracy: 0.9017 - val_loss: 0.3115 - val_accuracy: 0.8740 - lr: 1.0000e-05\n",
            "Epoch 2/25\n",
            "19/19 [==============================] - 11s 564ms/step - loss: 0.2631 - accuracy: 0.8817 - val_loss: 0.3059 - val_accuracy: 0.8760 - lr: 1.0000e-05\n",
            "Epoch 3/25\n",
            "19/19 [==============================] - 11s 573ms/step - loss: 0.2062 - accuracy: 0.9233 - val_loss: 0.3037 - val_accuracy: 0.8760 - lr: 1.0000e-05\n",
            "Epoch 4/25\n",
            "19/19 [==============================] - 10s 557ms/step - loss: 0.2350 - accuracy: 0.8933 - val_loss: 0.3032 - val_accuracy: 0.8800 - lr: 1.0000e-05\n",
            "Epoch 5/25\n",
            "19/19 [==============================] - 11s 561ms/step - loss: 0.2605 - accuracy: 0.8883 - val_loss: 0.3055 - val_accuracy: 0.8800 - lr: 1.0000e-05\n",
            "Epoch 6/25\n",
            "19/19 [==============================] - 11s 566ms/step - loss: 0.2728 - accuracy: 0.8800 - val_loss: 0.3068 - val_accuracy: 0.8760 - lr: 1.0000e-05\n",
            "Epoch 7/25\n",
            "19/19 [==============================] - 11s 569ms/step - loss: 0.2369 - accuracy: 0.8933 - val_loss: 0.3084 - val_accuracy: 0.8760 - lr: 1.0000e-05\n",
            "Epoch 8/25\n",
            "19/19 [==============================] - 10s 557ms/step - loss: 0.2200 - accuracy: 0.9067 - val_loss: 0.3089 - val_accuracy: 0.8760 - lr: 5.0000e-06\n",
            "Epoch 9/25\n",
            "19/19 [==============================] - 10s 553ms/step - loss: 0.3187 - accuracy: 0.8683 - val_loss: 0.3073 - val_accuracy: 0.8800 - lr: 5.0000e-06\n",
            "Epoch 10/25\n",
            "19/19 [==============================] - 10s 557ms/step - loss: 0.2627 - accuracy: 0.8933 - val_loss: 0.3092 - val_accuracy: 0.8760 - lr: 5.0000e-06\n",
            "Epoch 11/25\n",
            "19/19 [==============================] - 11s 561ms/step - loss: 0.2606 - accuracy: 0.8917 - val_loss: 0.3087 - val_accuracy: 0.8760 - lr: 2.5000e-06\n",
            "Epoch 12/25\n",
            "19/19 [==============================] - 11s 565ms/step - loss: 0.2291 - accuracy: 0.9133 - val_loss: 0.3092 - val_accuracy: 0.8740 - lr: 2.5000e-06\n",
            "Epoch 13/25\n",
            "19/19 [==============================] - 10s 556ms/step - loss: 0.2590 - accuracy: 0.8933 - val_loss: 0.3082 - val_accuracy: 0.8760 - lr: 2.5000e-06\n",
            "Epoch 14/25\n",
            "19/19 [==============================] - 11s 561ms/step - loss: 0.2591 - accuracy: 0.8950 - val_loss: 0.3087 - val_accuracy: 0.8760 - lr: 1.2500e-06\n",
            "Epoch 15/25\n",
            "19/19 [==============================] - 10s 554ms/step - loss: 0.2525 - accuracy: 0.8867 - val_loss: 0.3089 - val_accuracy: 0.8740 - lr: 1.2500e-06\n",
            "Epoch 16/25\n",
            "19/19 [==============================] - 10s 545ms/step - loss: 0.2475 - accuracy: 0.8867 - val_loss: 0.3094 - val_accuracy: 0.8740 - lr: 1.2500e-06\n",
            "Epoch 17/25\n",
            "19/19 [==============================] - 11s 559ms/step - loss: 0.2602 - accuracy: 0.8833 - val_loss: 0.3094 - val_accuracy: 0.8740 - lr: 1.0000e-06\n",
            "Epoch 18/25\n",
            "19/19 [==============================] - 10s 557ms/step - loss: 0.2912 - accuracy: 0.8750 - val_loss: 0.3095 - val_accuracy: 0.8740 - lr: 1.0000e-06\n",
            "Epoch 19/25\n",
            "19/19 [==============================] - 10s 552ms/step - loss: 0.2660 - accuracy: 0.8883 - val_loss: 0.3087 - val_accuracy: 0.8740 - lr: 1.0000e-06\n",
            "Epoch 20/25\n",
            "19/19 [==============================] - 11s 565ms/step - loss: 0.2334 - accuracy: 0.9100 - val_loss: 0.3082 - val_accuracy: 0.8760 - lr: 1.0000e-06\n",
            "Epoch 21/25\n",
            "19/19 [==============================] - 11s 557ms/step - loss: 0.2042 - accuracy: 0.9217 - val_loss: 0.3086 - val_accuracy: 0.8760 - lr: 1.0000e-06\n",
            "Epoch 22/25\n",
            "19/19 [==============================] - 11s 568ms/step - loss: 0.2512 - accuracy: 0.8917 - val_loss: 0.3083 - val_accuracy: 0.8740 - lr: 1.0000e-06\n",
            "Epoch 23/25\n",
            "19/19 [==============================] - 10s 551ms/step - loss: 0.2475 - accuracy: 0.8983 - val_loss: 0.3099 - val_accuracy: 0.8740 - lr: 1.0000e-06\n",
            "Epoch 24/25\n",
            "19/19 [==============================] - 10s 552ms/step - loss: 0.2554 - accuracy: 0.8817 - val_loss: 0.3099 - val_accuracy: 0.8740 - lr: 1.0000e-06\n",
            "Epoch 25/25\n",
            "19/19 [==============================] - 10s 549ms/step - loss: 0.2284 - accuracy: 0.9067 - val_loss: 0.3104 - val_accuracy: 0.8740 - lr: 1.0000e-06\n",
            "16/16 [==============================] - 4s 265ms/step - loss: 0.2870 - accuracy: 0.8840\n",
            "Test accuracy with pretrained model and training size (600 total images): 0.8840000033378601\n",
            "\n",
            "Training with pretrained model and sample size: 1200 (Total images: 1200)\n",
            "Found 1200 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "38/38 [==============================] - 18s 446ms/step - loss: 0.6241 - accuracy: 0.6975 - val_loss: 0.3627 - val_accuracy: 0.8460 - lr: 1.0000e-04\n",
            "Epoch 2/25\n",
            "38/38 [==============================] - 17s 435ms/step - loss: 0.4873 - accuracy: 0.7900 - val_loss: 0.3254 - val_accuracy: 0.8620 - lr: 1.0000e-04\n",
            "Epoch 3/25\n",
            "38/38 [==============================] - 17s 445ms/step - loss: 0.4876 - accuracy: 0.7917 - val_loss: 0.2772 - val_accuracy: 0.8700 - lr: 1.0000e-04\n",
            "Epoch 4/25\n",
            "38/38 [==============================] - 16s 432ms/step - loss: 0.4161 - accuracy: 0.8225 - val_loss: 0.2538 - val_accuracy: 0.8920 - lr: 1.0000e-04\n",
            "Epoch 5/25\n",
            "38/38 [==============================] - 16s 432ms/step - loss: 0.3690 - accuracy: 0.8333 - val_loss: 0.2794 - val_accuracy: 0.8680 - lr: 1.0000e-04\n",
            "Epoch 6/25\n",
            "38/38 [==============================] - 17s 435ms/step - loss: 0.3901 - accuracy: 0.8250 - val_loss: 0.2551 - val_accuracy: 0.8880 - lr: 1.0000e-04\n",
            "Epoch 7/25\n",
            "38/38 [==============================] - 17s 438ms/step - loss: 0.3446 - accuracy: 0.8450 - val_loss: 0.2440 - val_accuracy: 0.9080 - lr: 1.0000e-04\n",
            "Epoch 8/25\n",
            "38/38 [==============================] - 17s 442ms/step - loss: 0.3523 - accuracy: 0.8483 - val_loss: 0.2330 - val_accuracy: 0.9020 - lr: 1.0000e-04\n",
            "Epoch 9/25\n",
            "38/38 [==============================] - 17s 440ms/step - loss: 0.3774 - accuracy: 0.8358 - val_loss: 0.2250 - val_accuracy: 0.9100 - lr: 1.0000e-04\n",
            "Epoch 10/25\n",
            "38/38 [==============================] - 17s 437ms/step - loss: 0.3610 - accuracy: 0.8392 - val_loss: 0.2349 - val_accuracy: 0.9100 - lr: 1.0000e-04\n",
            "Epoch 11/25\n",
            "38/38 [==============================] - 17s 436ms/step - loss: 0.3420 - accuracy: 0.8483 - val_loss: 0.2271 - val_accuracy: 0.9040 - lr: 1.0000e-04\n",
            "Epoch 12/25\n",
            "38/38 [==============================] - 17s 435ms/step - loss: 0.3274 - accuracy: 0.8533 - val_loss: 0.2170 - val_accuracy: 0.9020 - lr: 1.0000e-04\n",
            "Epoch 13/25\n",
            "38/38 [==============================] - 17s 444ms/step - loss: 0.3278 - accuracy: 0.8700 - val_loss: 0.2361 - val_accuracy: 0.9060 - lr: 1.0000e-04\n",
            "Epoch 14/25\n",
            "38/38 [==============================] - 17s 435ms/step - loss: 0.3299 - accuracy: 0.8592 - val_loss: 0.1976 - val_accuracy: 0.9120 - lr: 1.0000e-04\n",
            "Epoch 15/25\n",
            "38/38 [==============================] - 17s 436ms/step - loss: 0.3385 - accuracy: 0.8550 - val_loss: 0.1990 - val_accuracy: 0.9120 - lr: 1.0000e-04\n",
            "Epoch 16/25\n",
            "38/38 [==============================] - 17s 436ms/step - loss: 0.2962 - accuracy: 0.8617 - val_loss: 0.2179 - val_accuracy: 0.9160 - lr: 1.0000e-04\n",
            "Epoch 17/25\n",
            "38/38 [==============================] - 17s 435ms/step - loss: 0.3208 - accuracy: 0.8625 - val_loss: 0.2224 - val_accuracy: 0.9000 - lr: 1.0000e-04\n",
            "Epoch 18/25\n",
            "38/38 [==============================] - 17s 441ms/step - loss: 0.3259 - accuracy: 0.8608 - val_loss: 0.2158 - val_accuracy: 0.9080 - lr: 5.0000e-05\n",
            "Epoch 19/25\n",
            "38/38 [==============================] - 17s 434ms/step - loss: 0.2917 - accuracy: 0.8692 - val_loss: 0.2060 - val_accuracy: 0.9120 - lr: 5.0000e-05\n",
            "Epoch 20/25\n",
            "38/38 [==============================] - 17s 435ms/step - loss: 0.3015 - accuracy: 0.8567 - val_loss: 0.2385 - val_accuracy: 0.9060 - lr: 5.0000e-05\n",
            "Epoch 21/25\n",
            "38/38 [==============================] - 17s 434ms/step - loss: 0.2892 - accuracy: 0.8658 - val_loss: 0.2124 - val_accuracy: 0.9120 - lr: 2.5000e-05\n",
            "Epoch 22/25\n",
            "38/38 [==============================] - 17s 439ms/step - loss: 0.3024 - accuracy: 0.8650 - val_loss: 0.2105 - val_accuracy: 0.9160 - lr: 2.5000e-05\n",
            "Epoch 23/25\n",
            "38/38 [==============================] - 17s 435ms/step - loss: 0.2944 - accuracy: 0.8742 - val_loss: 0.2130 - val_accuracy: 0.9180 - lr: 2.5000e-05\n",
            "Epoch 24/25\n",
            "38/38 [==============================] - 16s 431ms/step - loss: 0.2886 - accuracy: 0.8767 - val_loss: 0.2062 - val_accuracy: 0.9160 - lr: 1.2500e-05\n",
            "Epoch 25/25\n",
            "38/38 [==============================] - 17s 437ms/step - loss: 0.2586 - accuracy: 0.8908 - val_loss: 0.2091 - val_accuracy: 0.9160 - lr: 1.2500e-05\n",
            "Epoch 1/25\n",
            "38/38 [==============================] - 18s 445ms/step - loss: 0.2784 - accuracy: 0.8767 - val_loss: 0.2065 - val_accuracy: 0.9220 - lr: 1.0000e-05\n",
            "Epoch 2/25\n",
            "38/38 [==============================] - 17s 436ms/step - loss: 0.2864 - accuracy: 0.8683 - val_loss: 0.2027 - val_accuracy: 0.9160 - lr: 1.0000e-05\n",
            "Epoch 3/25\n",
            "38/38 [==============================] - 16s 432ms/step - loss: 0.2737 - accuracy: 0.8808 - val_loss: 0.1970 - val_accuracy: 0.9240 - lr: 1.0000e-05\n",
            "Epoch 4/25\n",
            "38/38 [==============================] - 17s 438ms/step - loss: 0.2985 - accuracy: 0.8675 - val_loss: 0.2002 - val_accuracy: 0.9200 - lr: 1.0000e-05\n",
            "Epoch 5/25\n",
            "38/38 [==============================] - 17s 437ms/step - loss: 0.2832 - accuracy: 0.8817 - val_loss: 0.1982 - val_accuracy: 0.9220 - lr: 1.0000e-05\n",
            "Epoch 6/25\n",
            "38/38 [==============================] - 17s 440ms/step - loss: 0.2652 - accuracy: 0.8783 - val_loss: 0.2018 - val_accuracy: 0.9200 - lr: 1.0000e-05\n",
            "Epoch 7/25\n",
            "38/38 [==============================] - 16s 432ms/step - loss: 0.2749 - accuracy: 0.8867 - val_loss: 0.1983 - val_accuracy: 0.9220 - lr: 5.0000e-06\n",
            "Epoch 8/25\n",
            "38/38 [==============================] - 17s 444ms/step - loss: 0.2839 - accuracy: 0.8892 - val_loss: 0.1986 - val_accuracy: 0.9220 - lr: 5.0000e-06\n",
            "Epoch 9/25\n",
            "38/38 [==============================] - 17s 438ms/step - loss: 0.2785 - accuracy: 0.8742 - val_loss: 0.1970 - val_accuracy: 0.9200 - lr: 5.0000e-06\n",
            "Epoch 10/25\n",
            "38/38 [==============================] - 17s 436ms/step - loss: 0.2867 - accuracy: 0.8775 - val_loss: 0.1972 - val_accuracy: 0.9220 - lr: 2.5000e-06\n",
            "Epoch 11/25\n",
            "38/38 [==============================] - 17s 437ms/step - loss: 0.2758 - accuracy: 0.8775 - val_loss: 0.1988 - val_accuracy: 0.9200 - lr: 2.5000e-06\n",
            "Epoch 12/25\n",
            "38/38 [==============================] - 16s 434ms/step - loss: 0.3036 - accuracy: 0.8617 - val_loss: 0.1973 - val_accuracy: 0.9200 - lr: 2.5000e-06\n",
            "Epoch 13/25\n",
            "38/38 [==============================] - 17s 438ms/step - loss: 0.2724 - accuracy: 0.8775 - val_loss: 0.1981 - val_accuracy: 0.9200 - lr: 1.2500e-06\n",
            "Epoch 14/25\n",
            "38/38 [==============================] - 17s 445ms/step - loss: 0.2878 - accuracy: 0.8833 - val_loss: 0.1988 - val_accuracy: 0.9200 - lr: 1.2500e-06\n",
            "Epoch 15/25\n",
            "38/38 [==============================] - 17s 434ms/step - loss: 0.2662 - accuracy: 0.8817 - val_loss: 0.2007 - val_accuracy: 0.9200 - lr: 1.2500e-06\n",
            "Epoch 16/25\n",
            "38/38 [==============================] - 17s 436ms/step - loss: 0.2784 - accuracy: 0.8767 - val_loss: 0.2007 - val_accuracy: 0.9200 - lr: 1.0000e-06\n",
            "Epoch 17/25\n",
            "38/38 [==============================] - 17s 436ms/step - loss: 0.2786 - accuracy: 0.8875 - val_loss: 0.1988 - val_accuracy: 0.9200 - lr: 1.0000e-06\n",
            "Epoch 18/25\n",
            "38/38 [==============================] - 16s 434ms/step - loss: 0.2626 - accuracy: 0.8833 - val_loss: 0.1990 - val_accuracy: 0.9200 - lr: 1.0000e-06\n",
            "Epoch 19/25\n",
            "38/38 [==============================] - 17s 434ms/step - loss: 0.2735 - accuracy: 0.8892 - val_loss: 0.2016 - val_accuracy: 0.9200 - lr: 1.0000e-06\n",
            "Epoch 20/25\n",
            "38/38 [==============================] - 17s 437ms/step - loss: 0.2700 - accuracy: 0.8775 - val_loss: 0.1999 - val_accuracy: 0.9200 - lr: 1.0000e-06\n",
            "Epoch 21/25\n",
            "38/38 [==============================] - 17s 436ms/step - loss: 0.3009 - accuracy: 0.8617 - val_loss: 0.2001 - val_accuracy: 0.9200 - lr: 1.0000e-06\n",
            "Epoch 22/25\n",
            "38/38 [==============================] - 17s 435ms/step - loss: 0.2887 - accuracy: 0.8783 - val_loss: 0.2006 - val_accuracy: 0.9200 - lr: 1.0000e-06\n",
            "Epoch 23/25\n",
            "38/38 [==============================] - 17s 437ms/step - loss: 0.2748 - accuracy: 0.8833 - val_loss: 0.2006 - val_accuracy: 0.9200 - lr: 1.0000e-06\n",
            "Epoch 24/25\n",
            "38/38 [==============================] - 16s 432ms/step - loss: 0.2728 - accuracy: 0.8808 - val_loss: 0.2028 - val_accuracy: 0.9180 - lr: 1.0000e-06\n",
            "Epoch 25/25\n",
            "38/38 [==============================] - 17s 441ms/step - loss: 0.2558 - accuracy: 0.8933 - val_loss: 0.2035 - val_accuracy: 0.9180 - lr: 1.0000e-06\n",
            "16/16 [==============================] - 4s 273ms/step - loss: 0.2369 - accuracy: 0.9000\n",
            "Test accuracy with pretrained model and training size (1200 total images): 0.8999999761581421\n",
            "\n",
            "Training with pretrained model and sample size: 3000 (Total images: 3000)\n",
            "Found 3000 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "94/94 [==============================] - 37s 374ms/step - loss: 0.5573 - accuracy: 0.7373 - val_loss: 0.3402 - val_accuracy: 0.8500 - lr: 1.0000e-04\n",
            "Epoch 2/25\n",
            "94/94 [==============================] - 35s 368ms/step - loss: 0.4411 - accuracy: 0.8057 - val_loss: 0.2703 - val_accuracy: 0.8860 - lr: 1.0000e-04\n",
            "Epoch 3/25\n",
            "94/94 [==============================] - 35s 368ms/step - loss: 0.4247 - accuracy: 0.8113 - val_loss: 0.2257 - val_accuracy: 0.9080 - lr: 1.0000e-04\n",
            "Epoch 4/25\n",
            "94/94 [==============================] - 34s 366ms/step - loss: 0.4173 - accuracy: 0.8197 - val_loss: 0.2073 - val_accuracy: 0.9080 - lr: 1.0000e-04\n",
            "Epoch 5/25\n",
            "94/94 [==============================] - 35s 370ms/step - loss: 0.3882 - accuracy: 0.8277 - val_loss: 0.1946 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
            "Epoch 6/25\n",
            "94/94 [==============================] - 35s 367ms/step - loss: 0.3741 - accuracy: 0.8407 - val_loss: 0.1983 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
            "Epoch 7/25\n",
            "94/94 [==============================] - 35s 368ms/step - loss: 0.3778 - accuracy: 0.8363 - val_loss: 0.1901 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
            "Epoch 8/25\n",
            "94/94 [==============================] - 34s 366ms/step - loss: 0.3848 - accuracy: 0.8360 - val_loss: 0.1936 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
            "Epoch 9/25\n",
            "94/94 [==============================] - 35s 370ms/step - loss: 0.3757 - accuracy: 0.8340 - val_loss: 0.1887 - val_accuracy: 0.9140 - lr: 1.0000e-04\n",
            "Epoch 10/25\n",
            "94/94 [==============================] - 35s 368ms/step - loss: 0.3547 - accuracy: 0.8457 - val_loss: 0.1798 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
            "Epoch 11/25\n",
            "94/94 [==============================] - 35s 369ms/step - loss: 0.3566 - accuracy: 0.8457 - val_loss: 0.1742 - val_accuracy: 0.9300 - lr: 1.0000e-04\n",
            "Epoch 12/25\n",
            "94/94 [==============================] - 35s 368ms/step - loss: 0.3654 - accuracy: 0.8387 - val_loss: 0.1625 - val_accuracy: 0.9320 - lr: 1.0000e-04\n",
            "Epoch 13/25\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 0.3472 - accuracy: 0.8447 - val_loss: 0.2389 - val_accuracy: 0.9000 - lr: 1.0000e-04\n",
            "Epoch 14/25\n",
            "94/94 [==============================] - 34s 366ms/step - loss: 0.3410 - accuracy: 0.8483 - val_loss: 0.1483 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
            "Epoch 15/25\n",
            "94/94 [==============================] - 35s 370ms/step - loss: 0.3256 - accuracy: 0.8557 - val_loss: 0.1698 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
            "Epoch 16/25\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 0.3383 - accuracy: 0.8517 - val_loss: 0.1786 - val_accuracy: 0.9280 - lr: 1.0000e-04\n",
            "Epoch 17/25\n",
            "94/94 [==============================] - 35s 372ms/step - loss: 0.3482 - accuracy: 0.8473 - val_loss: 0.2427 - val_accuracy: 0.8980 - lr: 1.0000e-04\n",
            "Epoch 18/25\n",
            "94/94 [==============================] - 35s 372ms/step - loss: 0.3146 - accuracy: 0.8647 - val_loss: 0.1419 - val_accuracy: 0.9440 - lr: 5.0000e-05\n",
            "Epoch 19/25\n",
            "94/94 [==============================] - 35s 369ms/step - loss: 0.3159 - accuracy: 0.8620 - val_loss: 0.1510 - val_accuracy: 0.9420 - lr: 5.0000e-05\n",
            "Epoch 20/25\n",
            "94/94 [==============================] - 35s 368ms/step - loss: 0.3167 - accuracy: 0.8617 - val_loss: 0.1683 - val_accuracy: 0.9340 - lr: 5.0000e-05\n",
            "Epoch 21/25\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 0.3259 - accuracy: 0.8617 - val_loss: 0.1515 - val_accuracy: 0.9320 - lr: 5.0000e-05\n",
            "Epoch 22/25\n",
            "94/94 [==============================] - 35s 372ms/step - loss: 0.3048 - accuracy: 0.8663 - val_loss: 0.1564 - val_accuracy: 0.9460 - lr: 2.5000e-05\n",
            "Epoch 23/25\n",
            "94/94 [==============================] - 35s 369ms/step - loss: 0.3182 - accuracy: 0.8610 - val_loss: 0.1401 - val_accuracy: 0.9480 - lr: 2.5000e-05\n",
            "Epoch 24/25\n",
            "94/94 [==============================] - 35s 373ms/step - loss: 0.3157 - accuracy: 0.8587 - val_loss: 0.1466 - val_accuracy: 0.9420 - lr: 2.5000e-05\n",
            "Epoch 25/25\n",
            "94/94 [==============================] - 35s 367ms/step - loss: 0.3234 - accuracy: 0.8577 - val_loss: 0.1372 - val_accuracy: 0.9460 - lr: 2.5000e-05\n",
            "Epoch 1/25\n",
            "94/94 [==============================] - 37s 380ms/step - loss: 0.3175 - accuracy: 0.8603 - val_loss: 0.1386 - val_accuracy: 0.9520 - lr: 1.0000e-05\n",
            "Epoch 2/25\n",
            "94/94 [==============================] - 35s 371ms/step - loss: 0.2942 - accuracy: 0.8727 - val_loss: 0.1429 - val_accuracy: 0.9500 - lr: 1.0000e-05\n",
            "Epoch 3/25\n",
            "94/94 [==============================] - 35s 368ms/step - loss: 0.3172 - accuracy: 0.8643 - val_loss: 0.1414 - val_accuracy: 0.9500 - lr: 1.0000e-05\n",
            "Epoch 4/25\n",
            "94/94 [==============================] - 35s 370ms/step - loss: 0.2984 - accuracy: 0.8750 - val_loss: 0.1401 - val_accuracy: 0.9500 - lr: 1.0000e-05\n",
            "Epoch 5/25\n",
            "94/94 [==============================] - 35s 368ms/step - loss: 0.2972 - accuracy: 0.8710 - val_loss: 0.1385 - val_accuracy: 0.9500 - lr: 5.0000e-06\n",
            "Epoch 6/25\n",
            "94/94 [==============================] - 35s 368ms/step - loss: 0.2975 - accuracy: 0.8743 - val_loss: 0.1400 - val_accuracy: 0.9480 - lr: 5.0000e-06\n",
            "Epoch 7/25\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 0.2919 - accuracy: 0.8707 - val_loss: 0.1409 - val_accuracy: 0.9520 - lr: 5.0000e-06\n",
            "Epoch 8/25\n",
            "94/94 [==============================] - 35s 373ms/step - loss: 0.3067 - accuracy: 0.8633 - val_loss: 0.1415 - val_accuracy: 0.9480 - lr: 5.0000e-06\n",
            "Epoch 9/25\n",
            "94/94 [==============================] - 35s 369ms/step - loss: 0.2789 - accuracy: 0.8770 - val_loss: 0.1415 - val_accuracy: 0.9500 - lr: 2.5000e-06\n",
            "Epoch 10/25\n",
            "94/94 [==============================] - 35s 370ms/step - loss: 0.3107 - accuracy: 0.8717 - val_loss: 0.1407 - val_accuracy: 0.9500 - lr: 2.5000e-06\n",
            "Epoch 11/25\n",
            "94/94 [==============================] - 35s 370ms/step - loss: 0.3009 - accuracy: 0.8700 - val_loss: 0.1397 - val_accuracy: 0.9480 - lr: 2.5000e-06\n",
            "Epoch 12/25\n",
            "94/94 [==============================] - 35s 367ms/step - loss: 0.3060 - accuracy: 0.8727 - val_loss: 0.1398 - val_accuracy: 0.9480 - lr: 1.2500e-06\n",
            "Epoch 13/25\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 0.2982 - accuracy: 0.8693 - val_loss: 0.1400 - val_accuracy: 0.9480 - lr: 1.2500e-06\n",
            "Epoch 14/25\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 0.3116 - accuracy: 0.8640 - val_loss: 0.1403 - val_accuracy: 0.9480 - lr: 1.2500e-06\n",
            "Epoch 15/25\n",
            "94/94 [==============================] - 35s 369ms/step - loss: 0.2973 - accuracy: 0.8707 - val_loss: 0.1409 - val_accuracy: 0.9480 - lr: 1.0000e-06\n",
            "Epoch 16/25\n",
            "94/94 [==============================] - 35s 369ms/step - loss: 0.2872 - accuracy: 0.8753 - val_loss: 0.1401 - val_accuracy: 0.9500 - lr: 1.0000e-06\n",
            "Epoch 17/25\n",
            "94/94 [==============================] - 34s 366ms/step - loss: 0.2952 - accuracy: 0.8703 - val_loss: 0.1416 - val_accuracy: 0.9480 - lr: 1.0000e-06\n",
            "Epoch 18/25\n",
            "94/94 [==============================] - 35s 367ms/step - loss: 0.3020 - accuracy: 0.8623 - val_loss: 0.1421 - val_accuracy: 0.9480 - lr: 1.0000e-06\n",
            "Epoch 19/25\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 0.2949 - accuracy: 0.8760 - val_loss: 0.1406 - val_accuracy: 0.9480 - lr: 1.0000e-06\n",
            "Epoch 20/25\n",
            "94/94 [==============================] - 35s 370ms/step - loss: 0.2796 - accuracy: 0.8800 - val_loss: 0.1404 - val_accuracy: 0.9480 - lr: 1.0000e-06\n",
            "Epoch 21/25\n",
            "94/94 [==============================] - 35s 369ms/step - loss: 0.3031 - accuracy: 0.8750 - val_loss: 0.1412 - val_accuracy: 0.9480 - lr: 1.0000e-06\n",
            "Epoch 22/25\n",
            "94/94 [==============================] - 35s 371ms/step - loss: 0.2965 - accuracy: 0.8683 - val_loss: 0.1410 - val_accuracy: 0.9480 - lr: 1.0000e-06\n",
            "Epoch 23/25\n",
            "94/94 [==============================] - 35s 370ms/step - loss: 0.2992 - accuracy: 0.8720 - val_loss: 0.1413 - val_accuracy: 0.9480 - lr: 1.0000e-06\n",
            "Epoch 24/25\n",
            "94/94 [==============================] - 35s 369ms/step - loss: 0.2935 - accuracy: 0.8693 - val_loss: 0.1401 - val_accuracy: 0.9480 - lr: 1.0000e-06\n",
            "Epoch 25/25\n",
            "94/94 [==============================] - 35s 367ms/step - loss: 0.3044 - accuracy: 0.8697 - val_loss: 0.1391 - val_accuracy: 0.9500 - lr: 1.0000e-06\n",
            "16/16 [==============================] - 4s 261ms/step - loss: 0.1464 - accuracy: 0.9420\n",
            "Test accuracy with pretrained model and training size (3000 total images): 0.9419999718666077\n",
            "\n",
            "Training with pretrained model and sample size: 6000 (Total images: 6000)\n",
            "Found 6000 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "188/188 [==============================] - 66s 346ms/step - loss: 0.5137 - accuracy: 0.7665 - val_loss: 0.2490 - val_accuracy: 0.8960 - lr: 1.0000e-04\n",
            "Epoch 2/25\n",
            "188/188 [==============================] - 65s 347ms/step - loss: 0.4148 - accuracy: 0.8157 - val_loss: 0.2661 - val_accuracy: 0.8840 - lr: 1.0000e-04\n",
            "Epoch 3/25\n",
            "188/188 [==============================] - 65s 346ms/step - loss: 0.4136 - accuracy: 0.8238 - val_loss: 0.2201 - val_accuracy: 0.9120 - lr: 1.0000e-04\n",
            "Epoch 4/25\n",
            "188/188 [==============================] - 64s 342ms/step - loss: 0.3954 - accuracy: 0.8257 - val_loss: 0.2146 - val_accuracy: 0.8880 - lr: 1.0000e-04\n",
            "Epoch 5/25\n",
            "188/188 [==============================] - 65s 344ms/step - loss: 0.3768 - accuracy: 0.8325 - val_loss: 0.2623 - val_accuracy: 0.8880 - lr: 1.0000e-04\n",
            "Epoch 6/25\n",
            "188/188 [==============================] - 65s 343ms/step - loss: 0.3577 - accuracy: 0.8402 - val_loss: 0.1949 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
            "Epoch 7/25\n",
            "188/188 [==============================] - 65s 347ms/step - loss: 0.3736 - accuracy: 0.8358 - val_loss: 0.2550 - val_accuracy: 0.8860 - lr: 1.0000e-04\n",
            "Epoch 8/25\n",
            "188/188 [==============================] - 64s 342ms/step - loss: 0.3620 - accuracy: 0.8430 - val_loss: 0.2260 - val_accuracy: 0.9120 - lr: 1.0000e-04\n",
            "Epoch 9/25\n",
            "188/188 [==============================] - 66s 349ms/step - loss: 0.3520 - accuracy: 0.8442 - val_loss: 0.1798 - val_accuracy: 0.9320 - lr: 1.0000e-04\n",
            "Epoch 10/25\n",
            "188/188 [==============================] - 65s 344ms/step - loss: 0.3613 - accuracy: 0.8383 - val_loss: 0.1863 - val_accuracy: 0.9300 - lr: 1.0000e-04\n",
            "Epoch 11/25\n",
            "188/188 [==============================] - 65s 345ms/step - loss: 0.3459 - accuracy: 0.8395 - val_loss: 0.1738 - val_accuracy: 0.9300 - lr: 1.0000e-04\n",
            "Epoch 12/25\n",
            "188/188 [==============================] - 65s 347ms/step - loss: 0.3434 - accuracy: 0.8510 - val_loss: 0.1792 - val_accuracy: 0.9320 - lr: 1.0000e-04\n",
            "Epoch 13/25\n",
            "188/188 [==============================] - 65s 344ms/step - loss: 0.3511 - accuracy: 0.8428 - val_loss: 0.1839 - val_accuracy: 0.9340 - lr: 1.0000e-04\n",
            "Epoch 14/25\n",
            "188/188 [==============================] - 65s 345ms/step - loss: 0.3367 - accuracy: 0.8492 - val_loss: 0.1619 - val_accuracy: 0.9340 - lr: 1.0000e-04\n",
            "Epoch 15/25\n",
            "188/188 [==============================] - 65s 343ms/step - loss: 0.3276 - accuracy: 0.8515 - val_loss: 0.1860 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
            "Epoch 16/25\n",
            "188/188 [==============================] - 65s 343ms/step - loss: 0.3325 - accuracy: 0.8555 - val_loss: 0.1747 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
            "Epoch 17/25\n",
            "188/188 [==============================] - 65s 347ms/step - loss: 0.3254 - accuracy: 0.8557 - val_loss: 0.1699 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
            "Epoch 18/25\n",
            "188/188 [==============================] - 64s 341ms/step - loss: 0.3243 - accuracy: 0.8535 - val_loss: 0.1773 - val_accuracy: 0.9360 - lr: 5.0000e-05\n",
            "Epoch 19/25\n",
            "188/188 [==============================] - 65s 347ms/step - loss: 0.3127 - accuracy: 0.8603 - val_loss: 0.1596 - val_accuracy: 0.9440 - lr: 5.0000e-05\n",
            "Epoch 20/25\n",
            "188/188 [==============================] - 65s 346ms/step - loss: 0.3151 - accuracy: 0.8590 - val_loss: 0.1635 - val_accuracy: 0.9360 - lr: 5.0000e-05\n",
            "Epoch 21/25\n",
            "188/188 [==============================] - 64s 342ms/step - loss: 0.3155 - accuracy: 0.8568 - val_loss: 0.1584 - val_accuracy: 0.9420 - lr: 5.0000e-05\n",
            "Epoch 22/25\n",
            "188/188 [==============================] - 65s 343ms/step - loss: 0.3194 - accuracy: 0.8580 - val_loss: 0.1528 - val_accuracy: 0.9520 - lr: 5.0000e-05\n",
            "Epoch 23/25\n",
            "188/188 [==============================] - 65s 344ms/step - loss: 0.3158 - accuracy: 0.8618 - val_loss: 0.1704 - val_accuracy: 0.9400 - lr: 5.0000e-05\n",
            "Epoch 24/25\n",
            "188/188 [==============================] - 66s 348ms/step - loss: 0.3165 - accuracy: 0.8600 - val_loss: 0.1516 - val_accuracy: 0.9380 - lr: 5.0000e-05\n",
            "Epoch 25/25\n",
            "188/188 [==============================] - 65s 346ms/step - loss: 0.3133 - accuracy: 0.8603 - val_loss: 0.1618 - val_accuracy: 0.9440 - lr: 5.0000e-05\n",
            "Epoch 1/25\n",
            "188/188 [==============================] - 66s 346ms/step - loss: 0.3231 - accuracy: 0.8542 - val_loss: 0.1579 - val_accuracy: 0.9360 - lr: 1.0000e-05\n",
            "Epoch 2/25\n",
            "188/188 [==============================] - 65s 344ms/step - loss: 0.3034 - accuracy: 0.8638 - val_loss: 0.1547 - val_accuracy: 0.9440 - lr: 1.0000e-05\n",
            "Epoch 3/25\n",
            "188/188 [==============================] - 64s 340ms/step - loss: 0.3016 - accuracy: 0.8633 - val_loss: 0.1518 - val_accuracy: 0.9420 - lr: 1.0000e-05\n",
            "Epoch 4/25\n",
            "188/188 [==============================] - 65s 346ms/step - loss: 0.3052 - accuracy: 0.8640 - val_loss: 0.1525 - val_accuracy: 0.9400 - lr: 1.0000e-05\n",
            "Epoch 5/25\n",
            "188/188 [==============================] - 65s 342ms/step - loss: 0.3149 - accuracy: 0.8618 - val_loss: 0.1525 - val_accuracy: 0.9460 - lr: 1.0000e-05\n",
            "Epoch 6/25\n",
            "188/188 [==============================] - 65s 343ms/step - loss: 0.3058 - accuracy: 0.8707 - val_loss: 0.1538 - val_accuracy: 0.9440 - lr: 1.0000e-05\n",
            "Epoch 7/25\n",
            "188/188 [==============================] - 65s 347ms/step - loss: 0.2999 - accuracy: 0.8695 - val_loss: 0.1540 - val_accuracy: 0.9440 - lr: 5.0000e-06\n",
            "Epoch 8/25\n",
            "188/188 [==============================] - 65s 345ms/step - loss: 0.3036 - accuracy: 0.8723 - val_loss: 0.1532 - val_accuracy: 0.9440 - lr: 5.0000e-06\n",
            "Epoch 9/25\n",
            "188/188 [==============================] - 65s 347ms/step - loss: 0.3013 - accuracy: 0.8685 - val_loss: 0.1516 - val_accuracy: 0.9440 - lr: 5.0000e-06\n",
            "Epoch 10/25\n",
            "188/188 [==============================] - 65s 345ms/step - loss: 0.3026 - accuracy: 0.8647 - val_loss: 0.1543 - val_accuracy: 0.9460 - lr: 5.0000e-06\n",
            "Epoch 11/25\n",
            "188/188 [==============================] - 65s 346ms/step - loss: 0.2972 - accuracy: 0.8753 - val_loss: 0.1536 - val_accuracy: 0.9420 - lr: 5.0000e-06\n",
            "Epoch 12/25\n",
            "188/188 [==============================] - 65s 344ms/step - loss: 0.3087 - accuracy: 0.8653 - val_loss: 0.1523 - val_accuracy: 0.9440 - lr: 5.0000e-06\n",
            "Epoch 13/25\n",
            "188/188 [==============================] - 65s 346ms/step - loss: 0.3034 - accuracy: 0.8703 - val_loss: 0.1507 - val_accuracy: 0.9460 - lr: 2.5000e-06\n",
            "Epoch 14/25\n",
            "188/188 [==============================] - 65s 344ms/step - loss: 0.3093 - accuracy: 0.8610 - val_loss: 0.1510 - val_accuracy: 0.9460 - lr: 2.5000e-06\n",
            "Epoch 15/25\n",
            "188/188 [==============================] - 65s 346ms/step - loss: 0.3049 - accuracy: 0.8652 - val_loss: 0.1515 - val_accuracy: 0.9440 - lr: 2.5000e-06\n",
            "Epoch 16/25\n",
            "188/188 [==============================] - 65s 346ms/step - loss: 0.3064 - accuracy: 0.8667 - val_loss: 0.1516 - val_accuracy: 0.9440 - lr: 2.5000e-06\n",
            "Epoch 17/25\n",
            "188/188 [==============================] - 65s 346ms/step - loss: 0.3119 - accuracy: 0.8630 - val_loss: 0.1502 - val_accuracy: 0.9460 - lr: 1.2500e-06\n",
            "Epoch 18/25\n",
            "188/188 [==============================] - 65s 347ms/step - loss: 0.3049 - accuracy: 0.8677 - val_loss: 0.1516 - val_accuracy: 0.9480 - lr: 1.2500e-06\n",
            "Epoch 19/25\n",
            "188/188 [==============================] - 65s 346ms/step - loss: 0.3092 - accuracy: 0.8670 - val_loss: 0.1507 - val_accuracy: 0.9460 - lr: 1.2500e-06\n",
            "Epoch 20/25\n",
            "188/188 [==============================] - 66s 348ms/step - loss: 0.3067 - accuracy: 0.8635 - val_loss: 0.1505 - val_accuracy: 0.9460 - lr: 1.2500e-06\n",
            "Epoch 21/25\n",
            "188/188 [==============================] - 65s 346ms/step - loss: 0.2990 - accuracy: 0.8713 - val_loss: 0.1502 - val_accuracy: 0.9460 - lr: 1.0000e-06\n",
            "Epoch 22/25\n",
            "188/188 [==============================] - 65s 346ms/step - loss: 0.2948 - accuracy: 0.8667 - val_loss: 0.1502 - val_accuracy: 0.9480 - lr: 1.0000e-06\n",
            "Epoch 23/25\n",
            "188/188 [==============================] - 65s 346ms/step - loss: 0.3019 - accuracy: 0.8658 - val_loss: 0.1507 - val_accuracy: 0.9460 - lr: 1.0000e-06\n",
            "Epoch 24/25\n",
            "188/188 [==============================] - 65s 347ms/step - loss: 0.3056 - accuracy: 0.8653 - val_loss: 0.1504 - val_accuracy: 0.9460 - lr: 1.0000e-06\n",
            "Epoch 25/25\n",
            "188/188 [==============================] - 66s 348ms/step - loss: 0.3032 - accuracy: 0.8663 - val_loss: 0.1497 - val_accuracy: 0.9460 - lr: 1.0000e-06\n",
            "16/16 [==============================] - 4s 268ms/step - loss: 0.1689 - accuracy: 0.9300\n",
            "Test accuracy with pretrained model and training size (6000 total images): 0.9300000071525574\n",
            "\n",
            "Results for Different Training Sizes (Pretrained Network):\n",
            "Training Size: 600 images, Test Accuracy: 0.8840000033378601\n",
            "Training Size: 1200 images, Test Accuracy: 0.8999999761581421\n",
            "Training Size: 3000 images, Test Accuracy: 0.9419999718666077\n",
            "Training Size: 6000 images, Test Accuracy: 0.9300000071525574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the pretrained VGG16 model, I tested various sample sizes to evaluate their effect on model performance. Here’s an expanded breakdown of the results and observations:\n",
        "\n",
        "600 Images (Total):\n",
        "With this small sample size, the pretrained VGG16 model reached an impressive 88.4% accuracy on the test set. This is significantly better than the accuracy achieved using a model trained from scratch with the same sample size, indicating that the pretrained model could generalize well even with limited data. Transfer learning allowed the model to leverage features learned from a large dataset (ImageNet), giving it a strong starting point even with fewer training images.\n",
        "\n",
        "1200 Images (Total):\n",
        "Doubling the sample size to 1200 images further improved performance, yielding a 90.0% accuracy on the test set. This accuracy increase suggests that the pretrained model could still benefit from additional data, enabling it to better adapt to the specific features of the Cats and Dogs dataset.\n",
        "\n",
        "3000 Images (Total):\n",
        "Using 3000 images provided a substantial improvement, leading to a peak accuracy of 94.2% on the test set. This was the highest accuracy achieved across all sample sizes with the pretrained model, indicating that 3000 images might be an optimal balance for this dataset. At this size, the model had sufficient data to fine-tune its weights effectively while still leveraging the pretrained layers' representations.\n",
        "\n",
        "6000 Images (Total):\n",
        "With the largest sample size, the pretrained model achieved 93.0% accuracy, which was slightly lower than with 3000 images. This result could be due to several factors, such as overfitting or reaching a plateau in model performance given the dataset characteristics. While 6000 images still provided high accuracy, it did not significantly improve over the 3000-image training set.\n",
        "\n",
        "Overall Observations:\n",
        "The pretrained model provided better performance across all sample sizes compared to training a model from scratch. The optimal test accuracy of 94.2% was achieved with a training sample size of 3000 images, suggesting that this sample size effectively balanced model complexity and available data. Pretrained models can greatly enhance performance on smaller datasets, as they retain useful features learned from large, diverse datasets, thus reducing the amount of data required to achieve high accuracy in new tasks"
      ],
      "metadata": {
        "id": "gANtLcafSf9J"
      }
    }
  ]
}